{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## RegNet Training"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import joblib\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision.models import regnet_y_800mf, RegNet_Y_800MF_Weights\n","from torchvision import transforms\n","from IPython.display import Audio\n","import librosa\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n","import torch.nn.functional as F\n","from torchaudio import functional as F_audio\n","from tqdm import tqdm\n","import timm\n","import torchaudio\n","\n","import copy\n","import random\n","import glob\n","import os\n","import time\n","import sys\n","import re\n","import pywt"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["RANDOM_SEED = 21\n","\n","# Set seed for experiment reproducibility\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","torch.cuda.manual_seed(RANDOM_SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We are running code on Localhost\n","We are using device: cpu\n"]}],"source":["## REUSE IN INFERENCE NOTEBOOK\n","\n","custom_dataset_path = '/kaggle/input/birdclef2023-inference'\n","if os.path.exists(os.path.join(custom_dataset_path, 'utils.py')):\n","    sys.path.append(custom_dataset_path)\n","else:\n","    sys.path.append('..')\n","import utils\n","\n","IS_IN_KAGGLE_ENV = utils.get_is_in_kaggle_env()\n","\n","DATA_PATH = '/kaggle/input/birdclef-2023' if IS_IN_KAGGLE_ENV else '../data'\n","JOBLIB_PATH = custom_dataset_path if IS_IN_KAGGLE_ENV else './'\n","\n","DEVICE = 'cpu' if IS_IN_KAGGLE_ENV else utils.determine_device()\n","\n","AUDIO_LENGTH_S = 5\n","SAMPLE_RATE = 32_000"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["if not IS_IN_KAGGLE_ENV and not os.path.exists(DATA_PATH):\n","    print(\"Downloading data ...\")\n","    !kaggle competitions download -c 'birdclef-2023'\n","    !mkdir ../data\n","    !unzip -q birdclef-2023.zip -d ../data\n","    !rm birdclef-2023.zip\n","\n","DF_METADATA_CSV = pd.read_csv(f\"{DATA_PATH}/train_metadata.csv\")\n","\n","AUDIO_DATA_DIR = f\"{DATA_PATH}/train_audio/\""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of rows with 1 sample: 7\n","Number of rows with 2 samples: 6\n","Number of rows with 1 sample: 0\n","Number of rows with 2 samples: 0\n"]}],"source":["# Rows with 1 sample: copy twice, rows with 2 samples: copy once\n","# This is to ensure that stratified sampling for train/valid/test splits works correctly\n","def ensure_min_two_samples(DF_METADATA_CSV):\n","    class_counts = DF_METADATA_CSV[\"primary_label\"].value_counts()\n","\n","    one_sample_rows = DF_METADATA_CSV[DF_METADATA_CSV[\"primary_label\"].isin(class_counts[class_counts == 1].index)]\n","    print(f\"Number of rows with 1 sample: {len(one_sample_rows)}\")\n","    if len(one_sample_rows) > 0:\n","        DF_METADATA_CSV = pd.concat([DF_METADATA_CSV, one_sample_rows, one_sample_rows], ignore_index=True)\n","    \n","    two_sample_rows = DF_METADATA_CSV[DF_METADATA_CSV[\"primary_label\"].isin(class_counts[class_counts == 2].index)]\n","    print(f\"Number of rows with 2 samples: {len(two_sample_rows)}\")\n","    if len(two_sample_rows) > 0:\n","        DF_METADATA_CSV = pd.concat([DF_METADATA_CSV, two_sample_rows], ignore_index=True)\n","\n","    return DF_METADATA_CSV\n","\n","DF_METADATA_CSV = ensure_min_two_samples(DF_METADATA_CSV)\n","\n","# Run again to verify\n","DF_METADATA_CSV = ensure_min_two_samples(DF_METADATA_CSV)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["## REUSE IN INFERENCE NOTEBOOK\n","\n","class BirdMelspecClf(nn.Module):\n","    def __init__(self, out_features, pretrained):\n","        super().__init__()\n","        \n","        # https://pytorch.org/vision/stable/models.html\n","\n","        self.regnet = regnet_y_800mf(\n","            weights=RegNet_Y_800MF_Weights.DEFAULT \\\n","                if pretrained else None\n","        )\n","\n","        \"\"\"\n","        Replace the stem to take 1 channel instead of 3. The original stem:\n","        RegnetCNN(\n","        (regnet): RegNet(\n","            (stem): SimpleStemIN(\n","            (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (2): ReLU(inplace=True)\n","        )\"\"\"\n","        self.regnet.stem[0] = nn.Conv2d(4, 32, kernel_size=3, stride=2, padding=1, bias=False)\n","        \n","        \n","        # Fine-tune the regnet classifier\n","        # self.regnet.fc = nn.Sequential(\n","        #     nn.Linear(self.regnet.fc.in_features, 512),\n","        #     nn.BatchNorm1d(512),\n","        #     nn.PReLU(),\n","\n","        #     nn.Linear(512, 256),\n","        #     nn.BatchNorm1d(256),\n","        #     nn.PReLU(),\n","            \n","        #     nn.Linear(256, out_features),\n","        # )\n","\n","        self.regnet.fc = nn.Linear(in_features=784, out_features=out_features, bias=True)\n","\n","        self.softmax = nn.Softmax(dim=1)\n"," \n","    def forward(self, x):\n","        logits = self.regnet(x)\n","        probas = self.softmax(logits)\n","\n","        return logits, probas\n","\n","\n","def get_model(out_features, device, pretrained=False, load_state_dict=True, state_dict_starts_with=f\"{AUDIO_LENGTH_S}s_model\"):\n","    model = BirdMelspecClf(out_features=out_features, pretrained=pretrained)\n","    print(f\"Loaded model {model.__class__.__name__} with {sum(p.numel() for p in model.parameters())} parameters, pretained={pretrained}\")\n","    model.to(device)\n","\n","    if not load_state_dict:\n","        return model\n","\n","    model_files = [f for f in os.listdir(JOBLIB_PATH) if f.startswith(state_dict_starts_with) and f.endswith('.pt')]\n","    if len(model_files) == 0:\n","        print(f\"No model starting with {state_dict_starts_with} found in {JOBLIB_PATH}\")\n","        return model\n","    \n","    # Extract timestamp from the filenames and sort based on it\n","    model_files.sort(key=lambda x: int(re.findall(r'\\d+', x)[-1]) if re.findall(r'\\d+', x) else -1)\n","\n","    # The latest model file is the last one in the sorted list\n","    latest_model_file = model_files[-1]\n","    model_path = os.path.join(JOBLIB_PATH, latest_model_file)\n","    model.load_state_dict(torch.load(model_path))\n","    print(f\"Loaded model weights from {model_path}\")\n","    model.to(device)\n","\n","    return model"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of classes: 264\n"]}],"source":["UNIQUE_CLASSES = DF_METADATA_CSV.primary_label.unique()\n","N_CLASSES = len(UNIQUE_CLASSES)\n","print(f\"Number of classes: {N_CLASSES}\")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["(264, '../data/train_audio\\\\abethr1')"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train_classes_paths = glob.glob(f\"{DATA_PATH}/train_audio/*\")\n","len(train_classes_paths), train_classes_paths[0]"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total samples in dataset: 16961\n"]}],"source":["print(f\"Total samples in dataset: {len(DF_METADATA_CSV)}\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Class with fewest samples: crefra2: 3 samples\n"]}],"source":["class_with_fewest_samples = DF_METADATA_CSV.primary_label.value_counts(sort=True).index[-1]\n","print(f\"Class with fewest samples: {class_with_fewest_samples}: {DF_METADATA_CSV.primary_label.value_counts().min()} samples\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["class WaveletTransformSingle(nn.Module):\n","  def __init__(\n","      self, \n","      wavelet: pywt.Wavelet,\n","      cut_to_nearest: int | None = None\n","  ):\n","    super(WaveletTransformSingle, self).__init__()\n","    self.wavelet = wavelet\n","    self.ctn = cut_to_nearest\n","\n","  def forward(self, X: torch.Tensor) -> torch.Tensor:\n","    item = X.cpu().numpy()\n","    \n","    wh, wl = pywt.dwt(item[0], self.wavelet)\n","    out = torch.stack((torch.from_numpy(wh), torch.from_numpy(wl)))\n","    \n","    if self.ctn is not None:\n","      out = out[:,:-1 * (out.shape[-1] % self.ctn)]\n","    \n","    return out"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["class AudioAugmentations(nn.Module):\n","  def __init__(\n","      self, \n","      stretch_rate=0.8,\n","      fixed_rate=True,\n","      freq_mask_param=30,\n","      time_mask_param=80\n","  ):\n","    super(AudioAugmentations, self).__init__()\n","    self.aug = nn.Sequential(\n","        torchaudio.transforms.TimeStretch(stretch_rate, fixed_rate=fixed_rate),\n","        torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask_param),\n","        torchaudio.transforms.TimeMasking(time_mask_param=time_mask_param),\n","    )\n","  \n","  def forward(self, X: torch.Tensor) -> torch.Tensor:\n","    return self.aug(X)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["## REUSE IN INFERENCE NOTEBOOK\n","\n","def resample(audio, current_sample_rate, desired_sample_rate=SAMPLE_RATE):\n","    resampler = torchaudio.transforms.Resample(orig_freq=current_sample_rate, new_freq=desired_sample_rate)\n","    resampled_audio = resampler(audio)\n","    return resampled_audio\n","\n","def load_audio(audio_path, sample_rate=SAMPLE_RATE):\n","    audio, sr = torchaudio.load(audio_path)\n","    if sr != sample_rate:\n","        audio = resample(audio, sr, sample_rate)\n","    return audio\n","\n","# Using librosa defaults for n_fft and hop_length\n","def get_melspec_transform(sample_rate=SAMPLE_RATE, n_fft=2048, hop_length=512, n_mels=128):\n","    return torchaudio.transforms.MelSpectrogram(\n","        sample_rate=sample_rate,\n","        n_fft=n_fft,\n","        hop_length=hop_length,\n","        n_mels=n_mels,\n","    )\n","\n","# Using librosa defaults for top_db\n","def get_melspec_db_transform(stype='power', top_db=80):\n","    return torchaudio.transforms.AmplitudeToDB(\n","        stype=stype,\n","        top_db=top_db\n","    )\n","\n","# Copied from torchaudio/transforms/_transforms.py (to avoid converting to melspec twice)\n","dct_mat = F_audio.create_dct(40, 128, \"ortho\")\n","def get_mfcc_from_melspec(melspec):\n","    return torch.matmul(melspec.transpose(-1, -2), dct_mat).transpose(-1, -2)\n","\n","def normalize_tensor(tensor):\n","    min_val = torch.min(tensor)\n","    max_val = torch.max(tensor)\n","    if max_val - min_val == 0:\n","        return tensor\n","    else:\n","        return (tensor - min_val) / (max_val - min_val)"]},{"cell_type":"code","execution_count":16,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-03-29T06:38:42.021281Z","iopub.status.busy":"2023-03-29T06:38:42.019762Z","iopub.status.idle":"2023-03-29T06:38:42.028449Z","shell.execute_reply":"2023-03-29T06:38:42.027144Z","shell.execute_reply.started":"2023-03-29T06:38:42.021210Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating label encoder...\n","Splitting dataframe into train 60%, valid 20%, test 20%, stratified by primary_label\n","Loaded model BirdMelspecClf with 5855040 parameters, pretained=True\n","batch_size: 8, data_percentage: 1, num_epochs: 20, n_mels: 128, learning_rate: 0.0001, pin_memory: True, \n","validate_on_train: True, device: cpu, pad_method: wrap, validate_train_pct: 0.333\n"]},{"name":"stderr","output_type":"stream","text":["Validation batches: 100%|██████████| 424/424 [05:48<00:00,  1.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Initial validation accuracy: 0.0590%\n","Starting epoch 1/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|██████████| 1272/1272 [33:35<00:00,  1.58s/it]\n","Validation batches: 100%|██████████| 423/423 [05:49<00:00,  1.21it/s]\n","Validation batches: 100%|██████████| 424/424 [05:45<00:00,  1.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e0_valacc4_traacc4_16847075700.pt ...\n","Finsished epoch 1/20. Train Loss: 0.6790, Valid Loss: 0.6827, Train Accuracy: 4.05%, Valid Accuracy: 3.69%\n","Starting epoch 2/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|██████████| 1272/1272 [31:29<00:00,  1.49s/it]\n","Validation batches: 100%|██████████| 423/423 [05:32<00:00,  1.27it/s]\n","Validation batches: 100%|██████████| 424/424 [05:38<00:00,  1.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e1_valacc8_traacc8_16847075701.pt ...\n","Finsished epoch 2/20. Train Loss: 0.6439, Valid Loss: 0.6537, Train Accuracy: 8.22%, Valid Accuracy: 7.90%\n","Starting epoch 3/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|██████████| 1272/1272 [31:39<00:00,  1.49s/it]\n","Validation batches: 100%|██████████| 423/423 [05:47<00:00,  1.22it/s]\n","Validation batches: 100%|██████████| 424/424 [06:04<00:00,  1.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e2_valacc13_traacc16_16847075702.pt ...\n","Finsished epoch 3/20. Train Loss: 0.5522, Valid Loss: 0.5806, Train Accuracy: 15.63%, Valid Accuracy: 13.47%\n","Starting epoch 4/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|██████████| 1272/1272 [32:17<00:00,  1.52s/it]\n","Validation batches: 100%|██████████| 423/423 [05:40<00:00,  1.24it/s]\n","Validation batches: 100%|██████████| 424/424 [05:39<00:00,  1.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e3_valacc23_traacc26_16847075703.pt ...\n","Finsished epoch 4/20. Train Loss: 0.4828, Valid Loss: 0.5339, Train Accuracy: 26.21%, Valid Accuracy: 23.17%\n","Starting epoch 5/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|██████████| 1272/1272 [31:42<00:00,  1.50s/it]\n","Validation batches: 100%|██████████| 423/423 [05:35<00:00,  1.26it/s]\n","Validation batches: 100%|██████████| 424/424 [05:39<00:00,  1.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e4_valacc25_traacc29_16847075704.pt ...\n","Finsished epoch 5/20. Train Loss: 0.4534, Valid Loss: 0.5037, Train Accuracy: 29.02%, Valid Accuracy: 25.32%\n","Starting epoch 6/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|██████████| 1272/1272 [32:08<00:00,  1.52s/it]\n","Validation batches: 100%|██████████| 423/423 [05:33<00:00,  1.27it/s]\n","Validation batches: 100%|██████████| 424/424 [05:39<00:00,  1.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e5_valacc29_traacc32_16847075705.pt ...\n","Finsished epoch 6/20. Train Loss: 0.4312, Valid Loss: 0.5155, Train Accuracy: 31.65%, Valid Accuracy: 29.16%\n","Starting epoch 7/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|██████████| 1272/1272 [31:59<00:00,  1.51s/it]\n","Validation batches: 100%|██████████| 423/423 [05:38<00:00,  1.25it/s]\n","Validation batches: 100%|██████████| 424/424 [05:42<00:00,  1.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e6_valacc34_traacc37_16847075706.pt ...\n","Finsished epoch 7/20. Train Loss: 0.3743, Valid Loss: 0.4881, Train Accuracy: 37.06%, Valid Accuracy: 33.58%\n","Starting epoch 8/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|██████████| 1272/1272 [32:45<00:00,  1.55s/it]\n","Validation batches: 100%|██████████| 423/423 [05:47<00:00,  1.22it/s]\n","Validation batches: 100%|██████████| 424/424 [05:48<00:00,  1.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e7_valacc35_traacc39_16847075707.pt ...\n","Finsished epoch 8/20. Train Loss: 0.3570, Valid Loss: 0.4596, Train Accuracy: 38.62%, Valid Accuracy: 34.61%\n","Starting epoch 9/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|██████████| 1272/1272 [33:50<00:00,  1.60s/it]\n","Validation batches: 100%|██████████| 423/423 [06:28<00:00,  1.09it/s]\n","Validation batches: 100%|██████████| 424/424 [06:03<00:00,  1.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e8_valacc36_traacc40_16847075708.pt ...\n","Finsished epoch 9/20. Train Loss: 0.3409, Valid Loss: 0.4516, Train Accuracy: 40.40%, Valid Accuracy: 35.97%\n","Starting epoch 10/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|██████████| 1272/1272 [35:38<00:00,  1.68s/it]\n","Validation batches: 100%|██████████| 423/423 [06:24<00:00,  1.10it/s]\n","Validation batches: 100%|██████████| 424/424 [06:25<00:00,  1.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Finsished epoch 10/20. Train Loss: 0.3447, Valid Loss: 0.4600, Train Accuracy: 41.34%, Valid Accuracy: 35.94%\n","Starting epoch 11/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|██████████| 1272/1272 [37:32<00:00,  1.77s/it]\n","Validation batches: 100%|██████████| 423/423 [06:48<00:00,  1.03it/s]\n","Validation batches: 100%|██████████| 424/424 [06:50<00:00,  1.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e10_valacc38_traacc43_168470757010.pt ...\n","Finsished epoch 11/20. Train Loss: 0.3253, Valid Loss: 0.4387, Train Accuracy: 43.11%, Valid Accuracy: 38.12%\n","Starting epoch 12/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches:  79%|███████▉  | 1002/1272 [31:41<08:32,  1.90s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 215\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39m# scheduler = None\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch_size: \u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m, data_percentage: \u001b[39m\u001b[39m{\u001b[39;00mdata_percentage\u001b[39m}\u001b[39;00m\u001b[39m, num_epochs: \u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m, n_mels: \u001b[39m\u001b[39m{\u001b[39;00mn_mels\u001b[39m}\u001b[39;00m\u001b[39m, learning_rate: \u001b[39m\u001b[39m{\u001b[39;00mlearning_rate\u001b[39m}\u001b[39;00m\u001b[39m, pin_memory: \u001b[39m\u001b[39m{\u001b[39;00mpin_memory\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mvalidate_on_train: \u001b[39m\u001b[39m{\u001b[39;00mvalidate_on_train\u001b[39m}\u001b[39;00m\u001b[39m, device: \u001b[39m\u001b[39m{\u001b[39;00mDEVICE\u001b[39m}\u001b[39;00m\u001b[39m, pad_method: \u001b[39m\u001b[39m{\u001b[39;00mpad_method\u001b[39m}\u001b[39;00m\u001b[39m, validate_train_pct: \u001b[39m\u001b[39m{\u001b[39;00mvalidate_train_pct\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 215\u001b[0m minibatch_loss, train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst \u001b[39m=\u001b[39m train(model, train_loader, valid_loader, loss_function, optimizer, num_epochs, validate_on_train, validate_train_pct, scheduler, DEVICE)\n","Cell \u001b[1;32mIn[16], line 109\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, valid_loader, loss_func, optimizer, num_epochs, validate_on_train, validate_train_pct, scheduler, device)\u001b[0m\n\u001b[0;32m    105\u001b[0m features \u001b[39m=\u001b[39m melspec\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    106\u001b[0m targets \u001b[39m=\u001b[39m primary_label\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m--> 109\u001b[0m logits, probas \u001b[39m=\u001b[39m model(features)\n\u001b[0;32m    111\u001b[0m loss \u001b[39m=\u001b[39m loss_func(logits, targets)\n\u001b[0;32m    113\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[1;32mIn[8], line 44\u001b[0m, in \u001b[0;36mBirdMelspecClf.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 44\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mregnet(x)\n\u001b[0;32m     45\u001b[0m     probas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(logits)\n\u001b[0;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m logits, probas\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torchvision\\models\\regnet.py:378\u001b[0m, in \u001b[0;36mRegNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    377\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstem(x)\n\u001b[1;32m--> 378\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrunk_output(x)\n\u001b[0;32m    380\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m    381\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torchvision\\models\\regnet.py:147\u001b[0m, in \u001b[0;36mResBottleneckBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    145\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf(x)\n\u001b[0;32m    146\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(x)\n\u001b[0;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["class BirdClef23Dataset(Dataset):\n","    def __init__(self, df, audio_data_dir, label_encoder, n_mels, device, pad_method='wrap'):\n","        self.df = df\n","        self.audio_data_dir = audio_data_dir\n","        self.label_encoder = label_encoder\n","        self.device = device\n","        self.pad_method = pad_method\n","        self.melspec_transform = get_melspec_transform(n_mels=n_mels)\n","        self.melspec_db_transform = get_melspec_db_transform()\n","        self.wave_transform = WaveletTransformSingle(pywt.Wavelet('sym4'))\n","        self.resize = transforms.Resize((128, 313), antialias=True)\n","\n","    def __getitem__(self, index):\n","        audio_path = os.path.join(self.audio_data_dir, self.df.iloc[index, 11])\n","        audio = load_audio(audio_path, SAMPLE_RATE)\n","\n","        # Increase audio length if below {AUDIO_LENGTH_S} by padding\n","        if audio.shape[1] < AUDIO_LENGTH_S * SAMPLE_RATE:\n","            padding_needed = AUDIO_LENGTH_S * SAMPLE_RATE - audio.shape[1]\n","\n","            if self.pad_method == 'wrap':\n","                audio = F.pad(audio, (0, padding_needed), mode='replicate')\n","\n","            elif self.pad_method == 'zeros':\n","                audio = F.pad(audio, (0, padding_needed), mode='constant', value=0)\n","\n","\n","        # Truncate audio length if above {AUDIO_LENGTH_S} by random cropping\n","        if audio.shape[1] > AUDIO_LENGTH_S * SAMPLE_RATE:\n","            max_start_idx = audio.shape[1] - (SAMPLE_RATE * AUDIO_LENGTH_S)\n","            start_idx = torch.randint(0, max_start_idx, (1,)).item()\n","            audio = audio[:, start_idx:start_idx + (SAMPLE_RATE * AUDIO_LENGTH_S)]\n","\n","        melspec = self.melspec_db_transform(self.melspec_transform(audio))\n","        norm_melspec = normalize_tensor(melspec)\n","\n","        melspec_wave = self.wave_transform(audio)\n","        wh, wl = melspec_wave[0], melspec_wave[1]\n","        wh_mel = self.melspec_db_transform(self.melspec_transform(wh))\n","        wl_mel = self.melspec_db_transform(self.melspec_transform(wl))\n","        norm_wh = normalize_tensor(self.resize(wh_mel.unsqueeze(0)))\n","        norm_wl = normalize_tensor(self.resize(wl_mel.unsqueeze(0)))\n","\n","        mfcc = get_mfcc_from_melspec(melspec)\n","        norm_mfcc = normalize_tensor(self.resize(mfcc))\n","\n","        # features = torch.cat((norm_melspec, norm_mfcc), dim=1)\n","        # features = norm_melspec\n","        features = torch.cat((norm_melspec, norm_wh, norm_wl, norm_mfcc), dim=0)\n","        \n","        primary_label_raw = self.df.iloc[index, 0]\n","        primary_label = self.label_encoder.transform([primary_label_raw])[0]\n","\n","        return features, primary_label\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","\n","def get_data_loader(dataset, batch_size=32, data_percentage=None, shuffle=False, pin_memory=False):\n","    if data_percentage is not None:\n","        data_len = int(len(dataset) * data_percentage)\n","        dataset, _ = random_split(dataset, [data_len, len(dataset) - data_len])\n","\n","    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n","\n","    return data_loader\n","\n","\n","def split_df(df, primary_label='primary_label', percentages=[60, 20, 20]):\n","    \"\"\"\n","    - Percentages: [train, valid, test]\n","    - Splits a dataframe into three dataframes (train, valid, test), stratified by primary_label\n","    - Also returns the class weights (based on the training set)\n","    \"\"\"\n","    print(f\"Splitting dataframe into train {percentages[0]}%, valid {percentages[1]}%, test {percentages[2]}%, stratified by {primary_label}\")\n","    \n","    train_perc, valid_perc, test_perc = [perc / 100 for perc in percentages]\n","    train_valid_split = round(train_perc / (train_perc + valid_perc), 2)\n","    \n","    temp_df, test_df = train_test_split(df, test_size=test_perc, stratify=df[primary_label], random_state=RANDOM_SEED)\n","    \n","    train_df, valid_df = train_test_split(temp_df, test_size=1-train_valid_split, stratify=temp_df[primary_label], random_state=RANDOM_SEED)\n","\n","    classes = np.unique(train_df[primary_label])\n","    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df[primary_label])\n","\n","    return train_df, valid_df, test_df, class_weights\n","\n","\n","def train(model, train_loader, valid_loader, loss_func, optimizer, num_epochs, validate_on_train, validate_train_pct, scheduler, device):\n","    minibatch_loss, train_acc_lst, valid_acc_lst, train_loss_lst, valid_loss_lst, best_valid_acc = [], [], [], [], [], 0.0\n","    train_start_ts = str(time.time()).split('.')[0]\n","\n","    # Initial validation step for models with loaded weights, so new weights are only saved if they improve on the loaded ones\n","    valid_acc, valid_loss = validate(model, device, valid_loader, loss_func)\n","    best_valid_acc = valid_acc\n","    print(f\"Initial validation accuracy: {valid_acc:.4f}%\")\n","    \n","    for epoch in range(num_epochs):\n","        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n","        model.train()\n","        \n","        for melspec, primary_label in tqdm(train_loader, total=len(train_loader), desc=\"Training batches\"):\n","            features = melspec.to(device)\n","            targets = primary_label.to(device=device, dtype=torch.long)\n","\n","\n","            logits, probas = model(features)\n","\n","            loss = loss_func(logits, targets)\n","\n","            optimizer.zero_grad()\n","            \n","            loss.backward()\n","\n","            minibatch_loss.append(loss.item())\n","            \n","            optimizer.step()\n","        \n","        train_acc, train_loss = validate(model, device, train_loader, loss_func, validate_train_pct) if validate_on_train else (torch.tensor(0.0), torch.tensor(0.0))\n","        valid_acc, valid_loss = validate(model, device, valid_loader, loss_func)\n","\n","        train_acc_lst.append(train_acc)\n","        train_loss_lst.append(train_loss)\n","        valid_acc_lst.append(valid_acc)\n","        valid_loss_lst.append(valid_loss)\n","\n","        if valid_acc > best_valid_acc:\n","            best_valid_acc = valid_acc\n","            model_name = f\"{AUDIO_LENGTH_S}s_model_e{epoch}_valacc{valid_acc:.0f}_traacc{train_acc:.0f}_{train_start_ts}{epoch}.pt\"\n","            print(f\"Moving model to CPU and saving it as {model_name} ...\")\n","            model_cpu = copy.deepcopy(model)\n","            model_cpu.to('cpu')\n","            model_path = os.path.join(JOBLIB_PATH, model_name)\n","            torch.save(model_cpu.state_dict(), model_path)\n","\n","        if scheduler is not None:\n","            lr_before = optimizer.param_groups[0]['lr']\n","            scheduler.step(valid_loss)\n","            lr_after = optimizer.param_groups[0]['lr']\n","            if lr_before != lr_after:\n","                print(f\"Learning rate changed from {lr_before} to {lr_after}\")\n","\n","        print(f\"Finsished epoch {epoch+1}/{num_epochs}. Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Valid Accuracy: {valid_acc:.2f}%\")\n","              \n","    return minibatch_loss, train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst\n","\n","\n","def validate(model, device, data_loader, loss_fn=F.cross_entropy, data_percentage=None):\n","    model.eval()\n","    \n","    num_examples, correct_pred, cross_entropy = 0.0, 0.0, 0.0\n","\n","    with torch.no_grad():\n","        data_loader_len = len(data_loader) if data_percentage is None else int(len(data_loader) * data_percentage)\n","        i = 0\n","        for melspec, primary_label in tqdm(data_loader, total=data_loader_len, desc='Validation batches'):\n","            if data_percentage is not None and i >= data_loader_len:\n","                break\n","            i += 1\n","\n","            features = melspec.to(device)\n","            targets = primary_label.to(device=device, dtype=torch.long)\n","\n","            logits, probas = model(features)\n","            cross_entropy += loss_fn(logits, targets)\n","\n","            _, predicted_labels = torch.max(probas, 1)\n","\n","            num_examples += targets.size(0)\n","\n","            correct_pred += (predicted_labels == targets).sum()\n","\n","    accuracy = correct_pred / num_examples * 100\n","    loss = cross_entropy / num_examples\n","    return accuracy, loss\n","\n","\n","# --- training\n","print('Creating label encoder...')\n","label_encoder = LabelEncoder()\n","label_encoder.fit(list(UNIQUE_CLASSES))\n","joblib.dump(label_encoder, 'label_encoder.joblib')\n","\n","train_df, valid_df, test_df, class_weights = split_df(DF_METADATA_CSV)\n","\n","batch_size = 8\n","data_percentage = 1 # 1 means 100% of the data\n","num_epochs = 20\n","n_mels = 128 # 128 is the default value in librosa\n","learning_rate = 0.0001\n","# DEVICE = 'cpu'\n","pad_method = 'wrap' # wrap, zeros\n","\n","pin_memory = True # https://discuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723/6\n","validate_on_train = True\n","validate_train_pct = 0.333\n","\n","train_dataset = BirdClef23Dataset(train_df, AUDIO_DATA_DIR, label_encoder, n_mels, DEVICE, pad_method=pad_method)\n","valid_dataset = BirdClef23Dataset(valid_df, AUDIO_DATA_DIR, label_encoder, n_mels, DEVICE, pad_method=pad_method)\n","\n","train_loader = get_data_loader(train_dataset, batch_size, data_percentage, shuffle=True, pin_memory=pin_memory)\n","valid_loader = get_data_loader(valid_dataset, batch_size, data_percentage, shuffle=False, pin_memory=pin_memory)\n","\n","model = get_model(out_features=N_CLASSES, device=DEVICE, pretrained=True, load_state_dict=False) # state_dict_starts_with='5s_model_e0_valacc6'\n","\n","loss_function = torch.nn.CrossEntropyLoss(weight=torch.from_numpy(class_weights).float().to(DEVICE))\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n","# scheduler = None\n","\n","print(f\"batch_size: {batch_size}, data_percentage: {data_percentage}, num_epochs: {num_epochs}, n_mels: {n_mels}, learning_rate: {learning_rate}, pin_memory: {pin_memory}, \\nvalidate_on_train: {validate_on_train}, device: {DEVICE}, pad_method: {pad_method}, validate_train_pct: {validate_train_pct}\")\n","\n","minibatch_loss, train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst = train(model, train_loader, valid_loader, loss_function, optimizer, num_epochs, validate_on_train, validate_train_pct, scheduler, DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utils.plot_minibatch_loss(minibatch_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utils.plot_train_and_valid_loss_and_accuracy(train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
