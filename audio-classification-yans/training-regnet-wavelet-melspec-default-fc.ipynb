{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## RegNet Training"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import joblib\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision.models import regnet_y_800mf, RegNet_Y_800MF_Weights\n","from torchvision import transforms\n","from IPython.display import Audio\n","import librosa\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n","import torch.nn.functional as F\n","from torchaudio import functional as F_audio\n","from tqdm import tqdm\n","import timm\n","import torchaudio\n","\n","import copy\n","import random\n","import glob\n","import os\n","import time\n","import sys\n","import re\n","import pywt"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["RANDOM_SEED = 21\n","\n","# Set seed for experiment reproducibility\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","torch.cuda.manual_seed(RANDOM_SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We are running code on Localhost\n","We are using device: cpu\n"]}],"source":["## REUSE IN INFERENCE NOTEBOOK\n","\n","custom_dataset_path = '/kaggle/input/birdclef2023-inference'\n","if os.path.exists(os.path.join(custom_dataset_path, 'utils.py')):\n","    sys.path.append(custom_dataset_path)\n","else:\n","    sys.path.append('..')\n","import utils\n","\n","IS_IN_KAGGLE_ENV = utils.get_is_in_kaggle_env()\n","\n","DATA_PATH = '/kaggle/input/birdclef-2023' if IS_IN_KAGGLE_ENV else '../data'\n","JOBLIB_PATH = custom_dataset_path if IS_IN_KAGGLE_ENV else './'\n","\n","DEVICE = 'cpu' if IS_IN_KAGGLE_ENV else utils.determine_device()\n","\n","AUDIO_LENGTH_S = 5\n","SAMPLE_RATE = 32_000"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["if not IS_IN_KAGGLE_ENV and not os.path.exists(DATA_PATH):\n","    print(\"Downloading data ...\")\n","    !kaggle competitions download -c 'birdclef-2023'\n","    !mkdir ../data\n","    !unzip -q birdclef-2023.zip -d ../data\n","    !rm birdclef-2023.zip\n","\n","DF_METADATA_CSV = pd.read_csv(f\"{DATA_PATH}/train_metadata.csv\")\n","\n","AUDIO_DATA_DIR = f\"{DATA_PATH}/train_audio/\""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of rows with 1 sample: 7\n","Number of rows with 2 samples: 6\n","Number of rows with 1 sample: 0\n","Number of rows with 2 samples: 0\n"]}],"source":["#Â Rows with 1 sample: copy twice, rows with 2 samples: copy once\n","# This is to ensure that stratified sampling for train/valid/test splits works correctly\n","def ensure_min_two_samples(DF_METADATA_CSV):\n","    class_counts = DF_METADATA_CSV[\"primary_label\"].value_counts()\n","\n","    one_sample_rows = DF_METADATA_CSV[DF_METADATA_CSV[\"primary_label\"].isin(class_counts[class_counts == 1].index)]\n","    print(f\"Number of rows with 1 sample: {len(one_sample_rows)}\")\n","    if len(one_sample_rows) > 0:\n","        DF_METADATA_CSV = pd.concat([DF_METADATA_CSV, one_sample_rows, one_sample_rows], ignore_index=True)\n","    \n","    two_sample_rows = DF_METADATA_CSV[DF_METADATA_CSV[\"primary_label\"].isin(class_counts[class_counts == 2].index)]\n","    print(f\"Number of rows with 2 samples: {len(two_sample_rows)}\")\n","    if len(two_sample_rows) > 0:\n","        DF_METADATA_CSV = pd.concat([DF_METADATA_CSV, two_sample_rows], ignore_index=True)\n","\n","    return DF_METADATA_CSV\n","\n","DF_METADATA_CSV = ensure_min_two_samples(DF_METADATA_CSV)\n","\n","# Run again to verify\n","DF_METADATA_CSV = ensure_min_two_samples(DF_METADATA_CSV)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["## REUSE IN INFERENCE NOTEBOOK\n","\n","class BirdMelspecClf(nn.Module):\n","    def __init__(self, out_features, pretrained):\n","        super().__init__()\n","        \n","        # https://pytorch.org/vision/stable/models.html\n","\n","        self.regnet = regnet_y_800mf(\n","            weights=RegNet_Y_800MF_Weights.DEFAULT \\\n","                if pretrained else None\n","        )\n","\n","        \"\"\"\n","        Replace the stem to take 1 channel instead of 3. The original stem:\n","        RegnetCNN(\n","        (regnet): RegNet(\n","            (stem): SimpleStemIN(\n","            (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (2): ReLU(inplace=True)\n","        )\"\"\"\n","        # self.regnet.stem[0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n","        \n","        \n","        # Fine-tune the regnet classifier\n","        # self.regnet.fc = nn.Sequential(\n","        #     nn.Linear(self.regnet.fc.in_features, 512),\n","        #     nn.BatchNorm1d(512),\n","        #     nn.PReLU(),\n","\n","        #     nn.Linear(512, 256),\n","        #     nn.BatchNorm1d(256),\n","        #     nn.PReLU(),\n","            \n","        #     nn.Linear(256, out_features),\n","        # )\n","\n","        self.regnet.fc = nn.Linear(in_features=784, out_features=out_features, bias=True)\n","\n","        self.softmax = nn.Softmax(dim=1)\n"," \n","    def forward(self, x):\n","        logits = self.regnet(x)\n","        probas = self.softmax(logits)\n","\n","        return logits, probas\n","\n","\n","def get_model(out_features, device, pretrained=False, load_state_dict=True, state_dict_starts_with=f\"{AUDIO_LENGTH_S}s_model\"):\n","    model = BirdMelspecClf(out_features=out_features, pretrained=pretrained)\n","    print(f\"Loaded model {model.__class__.__name__} with {sum(p.numel() for p in model.parameters())} parameters, pretained={pretrained}\")\n","    model.to(device)\n","\n","    if not load_state_dict:\n","        return model\n","\n","    model_files = [f for f in os.listdir(JOBLIB_PATH) if f.startswith(state_dict_starts_with) and f.endswith('.pt')]\n","    if len(model_files) == 0:\n","        print(f\"No model starting with {state_dict_starts_with} found in {JOBLIB_PATH}\")\n","        return model\n","    \n","    # Extract timestamp from the filenames and sort based on it\n","    model_files.sort(key=lambda x: int(re.findall(r'\\d+', x)[-1]) if re.findall(r'\\d+', x) else -1)\n","\n","    # The latest model file is the last one in the sorted list\n","    latest_model_file = model_files[-1]\n","    model_path = os.path.join(JOBLIB_PATH, latest_model_file)\n","    model.load_state_dict(torch.load(model_path))\n","    print(f\"Loaded model weights from {model_path}\")\n","    model.to(device)\n","\n","    return model"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of classes: 264\n"]}],"source":["UNIQUE_CLASSES = DF_METADATA_CSV.primary_label.unique()\n","N_CLASSES = len(UNIQUE_CLASSES)\n","print(f\"Number of classes: {N_CLASSES}\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["(264, '../data/train_audio\\\\abethr1')"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train_classes_paths = glob.glob(f\"{DATA_PATH}/train_audio/*\")\n","len(train_classes_paths), train_classes_paths[0]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total samples in dataset: 16961\n"]}],"source":["print(f\"Total samples in dataset: {len(DF_METADATA_CSV)}\")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Class with fewest samples: crefra2: 3 samples\n"]}],"source":["class_with_fewest_samples = DF_METADATA_CSV.primary_label.value_counts(sort=True).index[-1]\n","print(f\"Class with fewest samples: {class_with_fewest_samples}: {DF_METADATA_CSV.primary_label.value_counts().min()} samples\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class WaveletTransformSingle(nn.Module):\n","  def __init__(\n","      self, \n","      wavelet: pywt.Wavelet,\n","      cut_to_nearest: int | None = None\n","  ):\n","    super(WaveletTransformSingle, self).__init__()\n","    self.wavelet = wavelet\n","    self.ctn = cut_to_nearest\n","\n","  def forward(self, X: torch.Tensor) -> torch.Tensor:\n","    item = X.cpu().numpy()\n","    \n","    wh, wl = pywt.dwt(item[0], self.wavelet)\n","    out = torch.stack((torch.from_numpy(wh), torch.from_numpy(wl)))\n","    \n","    if self.ctn is not None:\n","      out = out[:,:-1 * (out.shape[-1] % self.ctn)]\n","    \n","    return out"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class AudioAugmentations(nn.Module):\n","  def __init__(\n","      self, \n","      stretch_rate=0.8,\n","      fixed_rate=True,\n","      freq_mask_param=30,\n","      time_mask_param=80\n","  ):\n","    super(AudioAugmentations, self).__init__()\n","    self.aug = nn.Sequential(\n","        torchaudio.transforms.TimeStretch(stretch_rate, fixed_rate=fixed_rate),\n","        torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask_param),\n","        torchaudio.transforms.TimeMasking(time_mask_param=time_mask_param),\n","    )\n","  \n","  def forward(self, X: torch.Tensor) -> torch.Tensor:\n","    return self.aug(X)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["## REUSE IN INFERENCE NOTEBOOK\n","\n","def resample(audio, current_sample_rate, desired_sample_rate=SAMPLE_RATE):\n","    resampler = torchaudio.transforms.Resample(orig_freq=current_sample_rate, new_freq=desired_sample_rate)\n","    resampled_audio = resampler(audio)\n","    return resampled_audio\n","\n","def load_audio(audio_path, sample_rate=SAMPLE_RATE):\n","    audio, sr = torchaudio.load(audio_path)\n","    if sr != sample_rate:\n","        audio = resample(audio, sr, sample_rate)\n","    return audio\n","\n","# Using librosa defaults for n_fft and hop_length\n","def get_melspec_transform(sample_rate=SAMPLE_RATE, n_fft=2048, hop_length=512, n_mels=128):\n","    return torchaudio.transforms.MelSpectrogram(\n","        sample_rate=sample_rate,\n","        n_fft=n_fft,\n","        hop_length=hop_length,\n","        n_mels=n_mels,\n","    )\n","\n","# Using librosa defaults for top_db\n","def get_melspec_db_transform(stype='power', top_db=80):\n","    return torchaudio.transforms.AmplitudeToDB(\n","        stype=stype,\n","        top_db=top_db\n","    )\n","\n","# Copied from torchaudio/transforms/_transforms.py (to avoid converting to melspec twice)\n","dct_mat = F_audio.create_dct(40, 128, \"ortho\")\n","def get_mfcc_from_melspec(melspec):\n","    return torch.matmul(melspec.transpose(-1, -2), dct_mat).transpose(-1, -2)\n","\n","def normalize_tensor(tensor):\n","    min_val = torch.min(tensor)\n","    max_val = torch.max(tensor)\n","    if max_val - min_val == 0:\n","        return tensor\n","    else:\n","        return (tensor - min_val) / (max_val - min_val)"]},{"cell_type":"code","execution_count":16,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-03-29T06:38:42.021281Z","iopub.status.busy":"2023-03-29T06:38:42.019762Z","iopub.status.idle":"2023-03-29T06:38:42.028449Z","shell.execute_reply":"2023-03-29T06:38:42.027144Z","shell.execute_reply.started":"2023-03-29T06:38:42.021210Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating label encoder...\n","Splitting dataframe into train 60%, valid 20%, test 20%, stratified by primary_label\n","Loaded model BirdMelspecClf with 5854752 parameters, pretained=True\n","batch_size: 8, data_percentage: 1, num_epochs: 20, n_mels: 128, learning_rate: 0.0001, pin_memory: True, \n","validate_on_train: True, device: cpu, pad_method: wrap, validate_train_pct: 0.333\n"]},{"name":"stderr","output_type":"stream","text":["Validation batches: 100%|ââââââââââ| 424/424 [05:06<00:00,  1.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Initial validation accuracy: 0.6486%\n","Starting epoch 1/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [30:56<00:00,  1.46s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [05:21<00:00,  1.32it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [05:17<00:00,  1.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e0_valacc12_traacc13_16844444170.pt ...\n","Finsished epoch 1/20. Train Loss: 0.6352, Valid Loss: 0.6443, Train Accuracy: 13.33%, Valid Accuracy: 12.00%\n","Starting epoch 2/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [29:50<00:00,  1.41s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [05:18<00:00,  1.33it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [05:15<00:00,  1.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e1_valacc29_traacc32_16844444171.pt ...\n","Finsished epoch 2/20. Train Loss: 0.4707, Valid Loss: 0.5191, Train Accuracy: 32.15%, Valid Accuracy: 28.51%\n","Starting epoch 3/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [29:14<00:00,  1.38s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [05:16<00:00,  1.34it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [05:15<00:00,  1.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e2_valacc38_traacc41_16844444172.pt ...\n","Finsished epoch 3/20. Train Loss: 0.3850, Valid Loss: 0.4522, Train Accuracy: 40.78%, Valid Accuracy: 37.59%\n","Starting epoch 4/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [29:18<00:00,  1.38s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [05:12<00:00,  1.35it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [05:15<00:00,  1.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e3_valacc40_traacc44_16844444173.pt ...\n","Finsished epoch 4/20. Train Loss: 0.3429, Valid Loss: 0.4322, Train Accuracy: 43.68%, Valid Accuracy: 39.92%\n","Starting epoch 5/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [29:16<00:00,  1.38s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [05:14<00:00,  1.35it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [05:15<00:00,  1.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e4_valacc44_traacc48_16844444174.pt ...\n","Finsished epoch 5/20. Train Loss: 0.3240, Valid Loss: 0.4130, Train Accuracy: 47.93%, Valid Accuracy: 44.25%\n","Starting epoch 6/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [29:20<00:00,  1.38s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [05:11<00:00,  1.36it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [05:16<00:00,  1.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e5_valacc45_traacc51_16844444175.pt ...\n","Finsished epoch 6/20. Train Loss: 0.2985, Valid Loss: 0.4136, Train Accuracy: 51.39%, Valid Accuracy: 45.05%\n","Starting epoch 7/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [29:55<00:00,  1.41s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [05:15<00:00,  1.34it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [05:15<00:00,  1.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e6_valacc46_traacc51_16844444176.pt ...\n","Finsished epoch 7/20. Train Loss: 0.2790, Valid Loss: 0.4013, Train Accuracy: 51.12%, Valid Accuracy: 45.75%\n","Starting epoch 8/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [29:56<00:00,  1.41s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [05:36<00:00,  1.26it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [05:17<00:00,  1.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e7_valacc46_traacc54_16844444177.pt ...\n","Finsished epoch 8/20. Train Loss: 0.2474, Valid Loss: 0.3974, Train Accuracy: 53.72%, Valid Accuracy: 45.93%\n","Starting epoch 9/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [30:17<00:00,  1.43s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [05:16<00:00,  1.34it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [05:23<00:00,  1.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Finsished epoch 9/20. Train Loss: 0.2815, Valid Loss: 0.4175, Train Accuracy: 50.56%, Valid Accuracy: 45.25%\n","Starting epoch 10/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [31:28<00:00,  1.48s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [05:25<00:00,  1.30it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [05:41<00:00,  1.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e9_valacc48_traacc54_16844444179.pt ...\n","Finsished epoch 10/20. Train Loss: 0.2574, Valid Loss: 0.4005, Train Accuracy: 54.14%, Valid Accuracy: 48.00%\n","Starting epoch 11/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [31:21<00:00,  1.48s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [05:28<00:00,  1.29it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [05:32<00:00,  1.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Finsished epoch 11/20. Train Loss: 0.2468, Valid Loss: 0.4089, Train Accuracy: 55.88%, Valid Accuracy: 47.32%\n","Starting epoch 12/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [31:56<00:00,  1.51s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [05:42<00:00,  1.24it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [05:41<00:00,  1.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_model_e11_valacc50_traacc59_168444441711.pt ...\n","Finsished epoch 12/20. Train Loss: 0.2354, Valid Loss: 0.3956, Train Accuracy: 59.07%, Valid Accuracy: 50.27%\n","Starting epoch 13/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [32:57<00:00,  1.55s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [05:51<00:00,  1.20it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [05:49<00:00,  1.21it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Finsished epoch 13/20. Train Loss: 0.2282, Valid Loss: 0.3903, Train Accuracy: 58.13%, Valid Accuracy: 49.68%\n","Starting epoch 14/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|ââââââââââ| 1272/1272 [34:28<00:00,  1.63s/it]\n","Validation batches: 100%|ââââââââââ| 423/423 [06:06<00:00,  1.15it/s]\n","Validation batches: 100%|ââââââââââ| 424/424 [06:06<00:00,  1.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Finsished epoch 14/20. Train Loss: 0.2469, Valid Loss: 0.4207, Train Accuracy: 56.26%, Valid Accuracy: 48.50%\n","Starting epoch 15/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches:  55%|ââââââ    | 701/1272 [20:14<16:29,  1.73s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 215\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39m# scheduler = None\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch_size: \u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m, data_percentage: \u001b[39m\u001b[39m{\u001b[39;00mdata_percentage\u001b[39m}\u001b[39;00m\u001b[39m, num_epochs: \u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m, n_mels: \u001b[39m\u001b[39m{\u001b[39;00mn_mels\u001b[39m}\u001b[39;00m\u001b[39m, learning_rate: \u001b[39m\u001b[39m{\u001b[39;00mlearning_rate\u001b[39m}\u001b[39;00m\u001b[39m, pin_memory: \u001b[39m\u001b[39m{\u001b[39;00mpin_memory\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mvalidate_on_train: \u001b[39m\u001b[39m{\u001b[39;00mvalidate_on_train\u001b[39m}\u001b[39;00m\u001b[39m, device: \u001b[39m\u001b[39m{\u001b[39;00mDEVICE\u001b[39m}\u001b[39;00m\u001b[39m, pad_method: \u001b[39m\u001b[39m{\u001b[39;00mpad_method\u001b[39m}\u001b[39;00m\u001b[39m, validate_train_pct: \u001b[39m\u001b[39m{\u001b[39;00mvalidate_train_pct\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 215\u001b[0m minibatch_loss, train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst \u001b[39m=\u001b[39m train(model, train_loader, valid_loader, loss_function, optimizer, num_epochs, validate_on_train, validate_train_pct, scheduler, DEVICE)\n","Cell \u001b[1;32mIn[16], line 115\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, valid_loader, loss_func, optimizer, num_epochs, validate_on_train, validate_train_pct, scheduler, device)\u001b[0m\n\u001b[0;32m    111\u001b[0m loss \u001b[39m=\u001b[39m loss_func(logits, targets)\n\u001b[0;32m    113\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 115\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    117\u001b[0m minibatch_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m    119\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["class BirdClef23Dataset(Dataset):\n","    def __init__(self, df, audio_data_dir, label_encoder, n_mels, device, pad_method='wrap'):\n","        self.df = df\n","        self.audio_data_dir = audio_data_dir\n","        self.label_encoder = label_encoder\n","        self.device = device\n","        self.pad_method = pad_method\n","        self.melspec_transform = get_melspec_transform(n_mels=n_mels)\n","        self.melspec_db_transform = get_melspec_db_transform()\n","        self.wave_transform = WaveletTransformSingle(pywt.Wavelet('sym4'))\n","        self.resize = transforms.Resize((128, 313), antialias=True)\n","\n","    def __getitem__(self, index):\n","        audio_path = os.path.join(self.audio_data_dir, self.df.iloc[index, 11])\n","        audio = load_audio(audio_path, SAMPLE_RATE)\n","\n","        # Increase audio length if below {AUDIO_LENGTH_S} by padding\n","        if audio.shape[1] < AUDIO_LENGTH_S * SAMPLE_RATE:\n","            padding_needed = AUDIO_LENGTH_S * SAMPLE_RATE - audio.shape[1]\n","\n","            if self.pad_method == 'wrap':\n","                audio = F.pad(audio, (0, padding_needed), mode='replicate')\n","\n","            elif self.pad_method == 'zeros':\n","                audio = F.pad(audio, (0, padding_needed), mode='constant', value=0)\n","\n","\n","        # Truncate audio length if above {AUDIO_LENGTH_S} by random cropping\n","        if audio.shape[1] > AUDIO_LENGTH_S * SAMPLE_RATE:\n","            max_start_idx = audio.shape[1] - (SAMPLE_RATE * AUDIO_LENGTH_S)\n","            start_idx = torch.randint(0, max_start_idx, (1,)).item()\n","            audio = audio[:, start_idx:start_idx + (SAMPLE_RATE * AUDIO_LENGTH_S)]\n","\n","        melspec = self.melspec_db_transform(self.melspec_transform(audio))\n","        norm_melspec = normalize_tensor(melspec)\n","\n","        melspec_wave = self.wave_transform(audio)\n","        wh, wl = melspec_wave[0], melspec_wave[1]\n","        wh_mel = self.melspec_db_transform(self.melspec_transform(wh))\n","        wl_mel = self.melspec_db_transform(self.melspec_transform(wl))\n","        norm_wh = normalize_tensor(self.resize(wh_mel.unsqueeze(0)))\n","        norm_wl = normalize_tensor(self.resize(wl_mel.unsqueeze(0)))\n","\n","        # mfcc = get_mfcc_from_melspec(melspec)\n","        # norm_mfcc = normalize_tensor(mfcc)\n","\n","        # features = torch.cat((norm_melspec, norm_mfcc), dim=1)\n","        # features = norm_melspec\n","        features = torch.cat((norm_melspec, norm_wh, norm_wl), dim=0)\n","        \n","        primary_label_raw = self.df.iloc[index, 0]\n","        primary_label = self.label_encoder.transform([primary_label_raw])[0]\n","\n","        return features, primary_label\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","\n","def get_data_loader(dataset, batch_size=32, data_percentage=None, shuffle=False, pin_memory=False):\n","    if data_percentage is not None:\n","        data_len = int(len(dataset) * data_percentage)\n","        dataset, _ = random_split(dataset, [data_len, len(dataset) - data_len])\n","\n","    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n","\n","    return data_loader\n","\n","\n","def split_df(df, primary_label='primary_label', percentages=[60, 20, 20]):\n","    \"\"\"\n","    - Percentages: [train, valid, test]\n","    - Splits a dataframe into three dataframes (train, valid, test), stratified by primary_label\n","    - Also returns the class weights (based on the training set)\n","    \"\"\"\n","    print(f\"Splitting dataframe into train {percentages[0]}%, valid {percentages[1]}%, test {percentages[2]}%, stratified by {primary_label}\")\n","    \n","    train_perc, valid_perc, test_perc = [perc / 100 for perc in percentages]\n","    train_valid_split = round(train_perc / (train_perc + valid_perc), 2)\n","    \n","    temp_df, test_df = train_test_split(df, test_size=test_perc, stratify=df[primary_label], random_state=RANDOM_SEED)\n","    \n","    train_df, valid_df = train_test_split(temp_df, test_size=1-train_valid_split, stratify=temp_df[primary_label], random_state=RANDOM_SEED)\n","\n","    classes = np.unique(train_df[primary_label])\n","    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df[primary_label])\n","\n","    return train_df, valid_df, test_df, class_weights\n","\n","\n","def train(model, train_loader, valid_loader, loss_func, optimizer, num_epochs, validate_on_train, validate_train_pct, scheduler, device):\n","    minibatch_loss, train_acc_lst, valid_acc_lst, train_loss_lst, valid_loss_lst, best_valid_acc = [], [], [], [], [], 0.0\n","    train_start_ts = str(time.time()).split('.')[0]\n","\n","    # Initial validation step for models with loaded weights, so new weights are only saved if they improve on the loaded ones\n","    valid_acc, valid_loss = validate(model, device, valid_loader, loss_func)\n","    best_valid_acc = valid_acc\n","    print(f\"Initial validation accuracy: {valid_acc:.4f}%\")\n","    \n","    for epoch in range(num_epochs):\n","        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n","        model.train()\n","        \n","        for melspec, primary_label in tqdm(train_loader, total=len(train_loader), desc=\"Training batches\"):\n","            features = melspec.to(device)\n","            targets = primary_label.to(device=device, dtype=torch.long)\n","\n","\n","            logits, probas = model(features)\n","\n","            loss = loss_func(logits, targets)\n","\n","            optimizer.zero_grad()\n","            \n","            loss.backward()\n","\n","            minibatch_loss.append(loss.item())\n","            \n","            optimizer.step()\n","        \n","        train_acc, train_loss = validate(model, device, train_loader, loss_func, validate_train_pct) if validate_on_train else (torch.tensor(0.0), torch.tensor(0.0))\n","        valid_acc, valid_loss = validate(model, device, valid_loader, loss_func)\n","\n","        train_acc_lst.append(train_acc)\n","        train_loss_lst.append(train_loss)\n","        valid_acc_lst.append(valid_acc)\n","        valid_loss_lst.append(valid_loss)\n","\n","        if valid_acc > best_valid_acc:\n","            best_valid_acc = valid_acc\n","            model_name = f\"{AUDIO_LENGTH_S}s_model_e{epoch}_valacc{valid_acc:.0f}_traacc{train_acc:.0f}_{train_start_ts}{epoch}.pt\"\n","            print(f\"Moving model to CPU and saving it as {model_name} ...\")\n","            model_cpu = copy.deepcopy(model)\n","            model_cpu.to('cpu')\n","            model_path = os.path.join(JOBLIB_PATH, model_name)\n","            torch.save(model_cpu.state_dict(), model_path)\n","\n","        if scheduler is not None:\n","            lr_before = optimizer.param_groups[0]['lr']\n","            scheduler.step(valid_loss)\n","            lr_after = optimizer.param_groups[0]['lr']\n","            if lr_before != lr_after:\n","                print(f\"Learning rate changed from {lr_before} to {lr_after}\")\n","\n","        print(f\"Finsished epoch {epoch+1}/{num_epochs}. Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Valid Accuracy: {valid_acc:.2f}%\")\n","              \n","    return minibatch_loss, train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst\n","\n","\n","def validate(model, device, data_loader, loss_fn=F.cross_entropy, data_percentage=None):\n","    model.eval()\n","    \n","    num_examples, correct_pred, cross_entropy = 0.0, 0.0, 0.0\n","\n","    with torch.no_grad():\n","        data_loader_len = len(data_loader) if data_percentage is None else int(len(data_loader) * data_percentage)\n","        i = 0\n","        for melspec, primary_label in tqdm(data_loader, total=data_loader_len, desc='Validation batches'):\n","            if data_percentage is not None and i >= data_loader_len:\n","                break\n","            i += 1\n","\n","            features = melspec.to(device)\n","            targets = primary_label.to(device=device, dtype=torch.long)\n","\n","            logits, probas = model(features)\n","            cross_entropy += loss_fn(logits, targets)\n","\n","            _, predicted_labels = torch.max(probas, 1)\n","\n","            num_examples += targets.size(0)\n","\n","            correct_pred += (predicted_labels == targets).sum()\n","\n","    accuracy = correct_pred / num_examples * 100\n","    loss = cross_entropy / num_examples\n","    return accuracy, loss\n","\n","\n","# --- training\n","print('Creating label encoder...')\n","label_encoder = LabelEncoder()\n","label_encoder.fit(list(UNIQUE_CLASSES))\n","joblib.dump(label_encoder, 'label_encoder.joblib')\n","\n","train_df, valid_df, test_df, class_weights = split_df(DF_METADATA_CSV)\n","\n","batch_size = 8\n","data_percentage = 1 # 1 means 100% of the data\n","num_epochs = 20\n","n_mels = 128 # 128 is the default value in librosa\n","learning_rate = 0.0001\n","# DEVICE = 'cpu'\n","pad_method = 'wrap' # wrap, zeros\n","\n","pin_memory = True # https://discuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723/6\n","validate_on_train = True\n","validate_train_pct = 0.333\n","\n","train_dataset = BirdClef23Dataset(train_df, AUDIO_DATA_DIR, label_encoder, n_mels, DEVICE, pad_method=pad_method)\n","valid_dataset = BirdClef23Dataset(valid_df, AUDIO_DATA_DIR, label_encoder, n_mels, DEVICE, pad_method=pad_method)\n","\n","train_loader = get_data_loader(train_dataset, batch_size, data_percentage, shuffle=True, pin_memory=pin_memory)\n","valid_loader = get_data_loader(valid_dataset, batch_size, data_percentage, shuffle=False, pin_memory=pin_memory)\n","\n","model = get_model(out_features=N_CLASSES, device=DEVICE, pretrained=False, load_state_dict=True) # state_dict_starts_with='5s_model_e0_valacc6'\n","\n","loss_function = torch.nn.CrossEntropyLoss(weight=torch.from_numpy(class_weights).float().to(DEVICE))\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n","# scheduler = None\n","\n","print(f\"batch_size: {batch_size}, data_percentage: {data_percentage}, num_epochs: {num_epochs}, n_mels: {n_mels}, learning_rate: {learning_rate}, pin_memory: {pin_memory}, \\nvalidate_on_train: {validate_on_train}, device: {DEVICE}, pad_method: {pad_method}, validate_train_pct: {validate_train_pct}\")\n","\n","minibatch_loss, train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst = train(model, train_loader, valid_loader, loss_function, optimizer, num_epochs, validate_on_train, validate_train_pct, scheduler, DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utils.plot_minibatch_loss(minibatch_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utils.plot_train_and_valid_loss_and_accuracy(train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
