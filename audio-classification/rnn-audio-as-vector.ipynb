{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Using an RNN classifier\n","\n","Results: ~3% accuracy\n","\n","Optimizations applied:\n","- initially I tried using an MLP, which led to problems due to the different lengths of the audio files, then I switched to an RNN (can handle variable length inputs)\n","- find a suitable minimum duration of the audio files: 24s (median)\n","- pad the audio files to the minimum length (repeat shorter files until they reach the median length)\n","- frame audio files to a multiple of {seconds}, because some audio files are much longer and it would be a waste of data to truncate them\n","- take class weights into account (the dataset is imbalanced)\n","- played with the hyperparameters of the RNN a bit\n","\n","Conclusion: This is a poor approach for the given problem. The RNN is not able to learn much from the audio files. Changing from 1s to 24s does not change the learning outcome significantly. \n","\n","Next: A spectrogram approach (computer vision)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import joblib\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision.models import resnet18, ResNet18_Weights\n","from torchvision import transforms\n","from IPython.display import Audio\n","import librosa\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n","import torch.nn.functional as F\n","\n","import random\n","import glob\n","import os\n","\n","import sys\n","sys.path.append(\"..\")\n","import utils"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["RANDOM_SEED = 21\n","\n","# Set seed for experiment reproducibility\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","torch.cuda.manual_seed(RANDOM_SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["is_in_kaggle_env = utils.get_is_in_kaggle_env()\n","\n","data_path = '/kaggle/input/birdclef-2023/' if is_in_kaggle_env else '../data/'\n","\n","device = 'cpu' if is_in_kaggle_env else utils.determine_device()\n","\n","if not is_in_kaggle_env and not os.path.exists('../data'):\n","    print(\"Downloading data...\")\n","    !kaggle competitions download -c 'birdclef-2023'\n","    !mkdir ../data\n","    !unzip -q birdclef-2023.zip -d ../data\n","    !rm birdclef-2023.zip\n","\n","df_metadata_csv = pd.read_csv(f\"{data_path}/train_metadata.csv\")\n","\n","audio_data_dir = f\"{data_path}/train_audio/\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class_counts = df_metadata_csv[\"primary_label\"].value_counts()\n","\n","two_or_less_samples_rows = df_metadata_csv[df_metadata_csv[\"primary_label\"].isin(class_counts[class_counts < 3].index)]\n","\n","print(f\"Number of unique classes with less than 2 samples: {len(two_or_less_samples_rows['primary_label'].unique())}\")\n","print(f\"Number of rows with less than 2 samples: {len(two_or_less_samples_rows)}\")\n","print(f\"Primary labels with less than 2 samples: {two_or_less_samples_rows['primary_label'].unique()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Drop rows with primary_label that have two or less samples\n","print(f\"Number of rows before dropping: {len(df_metadata_csv)}\")\n","df_metadata_csv = df_metadata_csv[~df_metadata_csv[\"primary_label\"].isin(class_counts[class_counts < 3].index)]\n","print(f\"Number of rows after dropping: {len(df_metadata_csv)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_classes = df_metadata_csv.primary_label.unique()\n","print(f\"Number of classes: {len(unique_classes)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-03-29T06:38:42.021281Z","iopub.status.busy":"2023-03-29T06:38:42.019762Z","iopub.status.idle":"2023-03-29T06:38:42.028449Z","shell.execute_reply":"2023-03-29T06:38:42.027144Z","shell.execute_reply.started":"2023-03-29T06:38:42.021210Z"},"trusted":true},"outputs":[],"source":["log_dims = False\n","\n","\n","class BirdClef23Dataset(Dataset):\n","    def __init__(self, df, audio_data_dir, label_encoder, seconds, hop_size_s=5.0):\n","        self.df = df\n","        self.audio_data_dir = audio_data_dir\n","        self.label_encoder = label_encoder\n","        self.seconds = seconds\n","        self.hop_size_s = hop_size_s\n","\n","    def __getitem__(self, index):\n","        if torch.is_tensor(index):\n","            index = index.tolist()\n","\n","        audio_path = os.path.join(self.audio_data_dir, self.df.iloc[index, 11])\n","        audio_numpy, audio_sr = librosa.load(audio_path, sr=32000)\n","\n","        if audio_sr != 32000:\n","            raise ValueError(f\"Sample rate is not 32000, it is {audio_sr} for {audio_path}\")\n","\n","        log_dims and print(f\"audio_numpy dims 0: {audio_numpy.shape}\")\n","\n","        # Artifically increase audio length if below {seconds}\n","        if audio_numpy.shape[0] < 32000 * self.seconds:\n","            log_dims and print(f\"Padding audio from {audio_numpy.shape[0]} to {32000 * self.seconds}\")\n","\n","            padding_needed = int(32000 * self.seconds - audio_numpy.shape[0])\n","            \n","            pad_width = (0, padding_needed)\n","            \n","            audio_numpy = np.pad(audio_numpy, pad_width, 'wrap')\n","\n","        log_dims and print(f\"audio_numpy dims 1: {audio_numpy.shape}\")\n","\n","        # Frame the audio (that means split it into windows of size {seconds} that overlap by {hop_size_s})\n","        frame_length = int(self.seconds * audio_sr)\n","        hop_length = int(self.hop_size_s * audio_sr)\n","        audio_numpy = librosa.util.frame(audio_numpy, frame_length=frame_length, hop_length=hop_length, axis=0)\n","\n","        log_dims and print(f\"audio_numpy dims 2: {audio_numpy.shape}\")\n","\n","        # Convert to tensor and add channel dimension\n","        audio_tensor = torch.from_numpy(audio_numpy.copy()).float().to(device)\n","        log_dims and print(f\"audio_tensor dims 0: {audio_tensor.shape}\")\n","\n","        audio_tensor = audio_tensor.unsqueeze(1)\n","        log_dims and print(f\"audio_tensor dims 1: {audio_tensor.shape}\\n\")\n","\n","        primary_label_raw = self.df.iloc[index, 0]\n","        primary_label = self.label_encoder.transform([primary_label_raw])[0]\n","\n","        row_id = audio_path.split('/')[-1].split('.')[0]\n","\n","        return row_id, audio_tensor, audio_numpy, primary_label\n","    \n","    def __len__(self):\n","        return len(self.df)\n","\n","\n","def get_data_loader(dataset, batch_size=32, data_percentage=None, shuffle=False):\n","    if data_percentage is not None:\n","        data_len = int(len(dataset) * data_percentage)\n","        dataset, _ = random_split(dataset, [data_len, len(dataset) - data_len])\n","\n","    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n","\n","    return data_loader\n","\n","\n","def split_df(df, primary_label='primary_label', percentages=[60, 20, 20]):\n","    \"\"\"\n","    - Percentages: [train, valid, test]\n","    - Splits a dataframe into three dataframes (train, valid, test), stratified by primary_label\n","    - Also returns the class weights (based on the training set)\n","    \"\"\"\n","    print(f\"Splitting dataframe into train {percentages[0]}%, valid {percentages[1]}%, test {percentages[2]}%, stratified by {primary_label}\")\n","    \n","    train_perc, valid_perc, test_perc = [perc / 100 for perc in percentages]\n","    train_valid_split = round(train_perc / (train_perc + valid_perc), 2)\n","    \n","    temp_df, test_df = train_test_split(df, test_size=test_perc, stratify=df[primary_label], random_state=RANDOM_SEED)\n","    \n","    train_df, valid_df = train_test_split(temp_df, test_size=1-train_valid_split, stratify=temp_df[primary_label], random_state=RANDOM_SEED)\n","\n","    classes = np.unique(train_df[primary_label])\n","    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df[primary_label])\n","\n","    return train_df, valid_df, test_df, class_weights\n","\n","\n","# class AudioClassifier(torch.nn.Module):\n","#     def __init__(self, num_classes, seconds):\n","#         super().__init__()\n","#         self.num_classes = num_classes\n","#         self.seconds = seconds\n","#         # 32000 because of the sampling rate\n","#         self.fc1 = torch.nn.Linear(32000 * self.seconds, 1000)\n","#         self.fc2 = torch.nn.Linear(1000, 100)\n","#         self.fc3 = torch.nn.Linear(100, self.num_classes)\n","#         self.sigmoid = torch.nn.Sigmoid()\n","\n","#     def forward(self, x):\n","#         x = x.view(-1, 32000 * self.seconds)\n","#         x = self.fc1(x)\n","#         x = self.fc2(x)\n","#         x = self.fc3(x)\n","#         x = self.sigmoid(x)\n","#         return x\n","\n","\n","class AudioRNN(torch.nn.Module):\n","    # RNN in pytorch: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html and https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n","    def __init__(self, num_classes, seconds, hidden_size):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.seconds = seconds\n","        self.hidden_size = hidden_size\n","        self.input_size = 32000 * self.seconds\n","        self.rnn = torch.nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, batch_first=True)\n","        self.fc = torch.nn.Linear(self.hidden_size, self.num_classes)\n","        self.softmax = torch.nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        x = x.view(batch_size, -1, self.input_size) # reshape input to batch_size x seq_len x input_size\n","        h0 = torch.zeros(1, batch_size, self.hidden_size).to(device)\n","        out, hn = self.rnn(x, h0) # pass input and hidden state through RNN\n","        logits = self.fc(out[:, -1, :])\n","        probas = self.softmax(logits)\n","        return logits, probas\n","\n","\n","def train(model, train_loader, valid_loader, loss_func, optimizer, num_epochs):\n","    minibatch_loss, train_acc_lst, valid_acc_lst, train_loss_lst, valid_loss_lst = [], [], [], [], []\n","    \n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","        model.train()\n","        \n","        for i, (row_id, audio_tensor, audio_numpy, primary_label) in enumerate(train_loader):\n","            print(f\"Batch {i + 1}/{len(train_loader)}\", end=\"\\r\")\n","\n","            features = audio_tensor.to(device)\n","            targets = primary_label.to(device)\n","\n","            log_dims and print(f\"Input tensor shape: {audio_tensor.shape}\")\n","            logits, probas = model(features)\n","\n","            loss = loss_func(logits, targets)\n","\n","            optimizer.zero_grad()\n","            \n","            loss.backward()\n","\n","            minibatch_loss.append(loss.item())\n","            \n","            optimizer.step()\n","            \n","        train_acc, train_loss = validate(model, train_loader, loss_func)\n","        valid_acc, valid_loss = validate(model, valid_loader, loss_func)\n","\n","        train_acc_lst.append(train_acc)\n","        valid_acc_lst.append(valid_acc)\n","        train_loss_lst.append(train_loss)\n","        valid_loss_lst.append(valid_loss)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs} done. Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Valid Accuracy: {valid_acc:.2f}%\")\n","              \n","    return minibatch_loss, train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst\n","\n","\n","def validate(model, data_loader, loss_fn=F.cross_entropy):\n","    model.eval()\n","    \n","    correct_pred, num_examples, cross_entropy = 0.0, 0.0, 0.0\n","\n","    with torch.no_grad():\n","        for row_id, audio_tensor, audio_numpy, primary_label in data_loader:\n","            features = audio_tensor.to(device)\n","            targets = primary_label.to(device)\n","\n","            logits, probas = model(features) # forward propagation z=logits, a=f(z)\n","            cross_entropy += loss_fn(logits, targets)\n","\n","            _, predicted_labels = torch.max(probas, 1)\n","            num_examples += targets.size(0)\n","\n","            correct_pred += (predicted_labels == targets).sum()\n","\n","    accuracy = correct_pred / num_examples * 100\n","    loss = cross_entropy / num_examples\n","    return accuracy, loss\n","\n","\n","# --- training\n","ignore_existing_label_encoder = True\n","if ignore_existing_label_encoder or not os.path.exists('label_encoder.joblib'):\n","    print('Creating label encoder...')\n","    label_encoder = LabelEncoder()\n","    label_encoder.fit(list(unique_classes))\n","    joblib.dump(label_encoder, 'label_encoder.joblib')\n","else:\n","    print('Loading label encoder...')\n","    label_encoder = joblib.load('label_encoder.joblib')\n","\n","train_df, valid_df, test_df, class_weights = split_df(df_metadata_csv)\n","\n","seconds = 10\n","batch_size = 1 # otherwise the training will fail because of the varying length of the audio files, which results in varying shape of tensors due to framing the audio\n","data_percentage = 1 # 1 means 100% of the data\n","num_epochs = 2\n","\n","train_dataset = BirdClef23Dataset(train_df, audio_data_dir, label_encoder, seconds)\n","valid_dataset = BirdClef23Dataset(valid_df, audio_data_dir, label_encoder, seconds)\n","test_dataset = BirdClef23Dataset(test_df, audio_data_dir, label_encoder, seconds)\n","\n","train_loader = get_data_loader(train_dataset, batch_size, data_percentage, shuffle=True)\n","valid_loader = get_data_loader(valid_dataset, batch_size, data_percentage, shuffle=False)\n","test_loader = get_data_loader(test_dataset, batch_size, data_percentage, shuffle=False)\n","\n","# model = AudioClassifier(num_classes=len(unique_classes), seconds=seconds).to(device)\n","model = AudioRNN(num_classes=len(unique_classes), seconds=seconds, hidden_size=128).to(device)\n","print(f\"Initialized model {model._get_name()}\")\n","\n","loss_function = torch.nn.CrossEntropyLoss(weight=torch.from_numpy(class_weights).float().to(device))\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n","\n","minibatch_loss, train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst = train(model, train_loader, valid_loader, loss_function, optimizer, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utils.plot_minibatch_loss(minibatch_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utils.plot_train_and_valid_loss_and_accuracy(train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get first item from train loader\n","row_id, audio_tensor, audio_numpy, primary_label = next(iter(train_loader))\n","print(f\"primary_label: {primary_label}, row_id: {row_id}\")\n","\n","class_name = label_encoder.inverse_transform(primary_label)\n","print(f\"class_name: {class_name}\")\n","\n","df_metadata_csv[df_metadata_csv['filename'].str.contains(\"XC321277\")]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
