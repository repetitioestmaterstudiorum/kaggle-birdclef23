{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## RegNetY_800MF CNN transfer learning, audio files as spectrograms\n","\n","https://pytorch.org/vision/stable/models/generated/torchvision.models.regnet_y_800mf.html#torchvision.models.RegNet_Y_800MF_Weights\n","\n","**Results**: 48%, still learning after epoch 5 (tuning potential).\n","\n","**Conclusion**: Due to the low amount of trainable parameters and yet great results, this could be the ideal CNN for this problem.\n","\n","**Next**: Feature preparation because it looks like there's still lots of noise in the data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import joblib\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision.models import regnet_y_800mf, RegNet_Y_800MF_Weights\n","from torchvision import transforms\n","from IPython.display import Audio\n","import librosa\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","\n","import random\n","import glob\n","import os\n","import time\n","\n","import sys\n","sys.path.append(\"..\")\n","import utils"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["RANDOM_SEED = 21\n","\n","# Set seed for experiment reproducibility\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","torch.cuda.manual_seed(RANDOM_SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["is_in_kaggle_env = utils.get_is_in_kaggle_env()\n","\n","data_path = '/kaggle/input/birdclef-2023' if is_in_kaggle_env else '../data'\n","\n","device = 'cpu' if is_in_kaggle_env else utils.determine_device()\n","\n","if not is_in_kaggle_env and not os.path.exists('../data'):\n","    print(\"Downloading data...\")\n","    !kaggle competitions download -c 'birdclef-2023'\n","    !mkdir ../data\n","    !unzip -q birdclef-2023.zip -d ../data\n","    !rm birdclef-2023.zip\n","\n","df_metadata_csv = pd.read_csv(f\"{data_path}/train_metadata.csv\")\n","\n","audio_data_dir = f\"{data_path}/train_audio/\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class_counts = df_metadata_csv[\"primary_label\"].value_counts()\n","\n","two_or_less_samples_rows = df_metadata_csv[df_metadata_csv[\"primary_label\"].isin(class_counts[class_counts < 3].index)]\n","\n","print(f\"Number of unique classes with less than 2 samples: {len(two_or_less_samples_rows['primary_label'].unique())}\")\n","print(f\"Number of rows with less than 2 samples: {len(two_or_less_samples_rows)}\")\n","print(f\"Primary labels with less than 2 samples: {two_or_less_samples_rows['primary_label'].unique()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Drop rows with primary_label that have two or less samples\n","print(f\"Number of rows before dropping: {len(df_metadata_csv)}\")\n","df_metadata_csv = df_metadata_csv[~df_metadata_csv[\"primary_label\"].isin(class_counts[class_counts < 3].index)]\n","print(f\"Number of rows after dropping: {len(df_metadata_csv)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_classes = df_metadata_csv.primary_label.unique()\n","print(f\"Number of classes: {len(unique_classes)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-03-29T06:38:42.021281Z","iopub.status.busy":"2023-03-29T06:38:42.019762Z","iopub.status.idle":"2023-03-29T06:38:42.028449Z","shell.execute_reply":"2023-03-29T06:38:42.027144Z","shell.execute_reply.started":"2023-03-29T06:38:42.021210Z"},"trusted":true},"outputs":[],"source":["class BirdClef23Dataset(Dataset):\n","    def __init__(self, df, audio_data_dir, label_encoder, seconds, n_mels, device, pad_method = 'wrap'):\n","        self.df = df\n","        self.audio_data_dir = audio_data_dir\n","        self.label_encoder = label_encoder\n","        self.seconds = seconds\n","        self.n_mels = n_mels\n","        self.device = device\n","        self.pad_method = pad_method\n","\n","    def __getitem__(self, index):\n","        audio_path = os.path.join(self.audio_data_dir, self.df.iloc[index, 11])\n","        audio_numpy, audio_sr = librosa.load(audio_path, sr=32000)\n","\n","        if audio_sr != 32000:\n","            raise ValueError(f\"Sample rate is not 32000, it is {audio_sr} for {audio_path}\")\n","\n","        # Increase audio length if below {seconds} by padding\n","        if audio_numpy.shape[0] < 32000 * self.seconds:\n","            padding_needed = int(32000 * self.seconds - audio_numpy.shape[0])\n","            \n","            pad_width = (0, padding_needed)\n","            \n","            # wrap means copy the audio until the length is reached\n","            if self.pad_method in ['wrap', 'wrap_double_reflect']:\n","                audio_numpy = np.pad(audio_numpy, pad_width, 'wrap')\n","\n","            #Â constant means pad with constant value, here 0 --> 1-2% less accuracy\n","            if self.pad_method == 'constant_zero':\n","                audio_numpy = np.pad(audio_numpy, pad_width, 'constant', constant_values=0)\n","\n","            # reflect means the vector mirrored\n","            if self.pad_method == 'reflect':\n","                audio_numpy = np.pad(audio_numpy, pad_width, 'reflect')\n","\n","        # Truncate audio length if above {seconds}\n","        if audio_numpy.shape[0] > 32000 * self.seconds:\n","            audio_numpy = audio_numpy[:32000 * self.seconds]\n","            \n","            # max_start_idx = audio_numpy.shape[0] - (32000 * self.seconds)\n","            # start_idx = np.random.randint(0, max_start_idx)\n","            # audio_numpy = audio_numpy[start_idx:start_idx + (32000 * self.seconds)]\n","\n","        # Create a mirrored version of the audio_numpy array and concatenate it\n","        if self.pad_method == 'wrap_double_reflect':\n","            mirrored_audio_numpy = audio_numpy[::-1]\n","            audio_numpy = np.concatenate((audio_numpy, mirrored_audio_numpy))\n","\n","        # What is a mel-scaled spectrogram? https://www.youtube.com/watch?v=PYlr8ayHb4g\n","        # Compute mel-scaled spectrogram and convert to log scale (dB) https://librosa.org/doc/latest/generated/librosa.feature.melspectrogram.html\n","        mel_spectrogram = librosa.feature.melspectrogram(y=audio_numpy, sr=audio_sr, n_mels=self.n_mels)\n","        log_mel_spectrogram = librosa.amplitude_to_db(mel_spectrogram)\n","        log_mel_spectrogram_norm = utils.normalize_spectrogram(log_mel_spectrogram)\n","\n","        audio_numpy = log_mel_spectrogram_norm.reshape((1, log_mel_spectrogram_norm.shape[0], log_mel_spectrogram_norm.shape[1]))\n","        \n","        audio_tensor = torch.from_numpy(audio_numpy).float().to(self.device)\n","\n","        primary_label_raw = self.df.iloc[index, 0]\n","        primary_label = self.label_encoder.transform([primary_label_raw])[0]\n","\n","        row_id = audio_path.split('/')[-1].split('.')[0]\n","\n","        return row_id, audio_tensor, primary_label\n","    \n","    def __len__(self):\n","        return len(self.df)\n","\n","\n","def get_data_loader(dataset, batch_size=32, data_percentage=None, shuffle=False, pin_memory=False):\n","    if data_percentage is not None:\n","        data_len = int(len(dataset) * data_percentage)\n","        dataset, _ = random_split(dataset, [data_len, len(dataset) - data_len])\n","\n","    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n","\n","    return data_loader\n","\n","\n","def split_df(df, primary_label='primary_label', percentages=[60, 20, 20]):\n","    \"\"\"\n","    - Percentages: [train, valid, test]\n","    - Splits a dataframe into three dataframes (train, valid, test), stratified by primary_label\n","    - Also returns the class weights (based on the training set)\n","    \"\"\"\n","    print(f\"Splitting dataframe into train {percentages[0]}%, valid {percentages[1]}%, test {percentages[2]}%, stratified by {primary_label}\")\n","    \n","    train_perc, valid_perc, test_perc = [perc / 100 for perc in percentages]\n","    train_valid_split = round(train_perc / (train_perc + valid_perc), 2)\n","    \n","    temp_df, test_df = train_test_split(df, test_size=test_perc, stratify=df[primary_label], random_state=RANDOM_SEED)\n","    \n","    train_df, valid_df = train_test_split(temp_df, test_size=1-train_valid_split, stratify=temp_df[primary_label], random_state=RANDOM_SEED)\n","\n","    classes = np.unique(train_df[primary_label])\n","    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df[primary_label])\n","\n","    return train_df, valid_df, test_df, class_weights\n","\n","\n","class RegnetCNN(torch.nn.Module):\n","    def __init__(self, num_classes):\n","        super().__init__()\n","        \n","        # https://pytorch.org/vision/stable/models.html\n","        self.regnet = regnet_y_800mf(weights=RegNet_Y_800MF_Weights.DEFAULT)\n","\n","        \"\"\"\n","        Replace the stem to take 1 channel instead of 3. The original stem:\n","        RegnetCNN(\n","        (regnet): RegNet(\n","            (stem): SimpleStemIN(\n","            (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (2): ReLU(inplace=True)\n","        )\"\"\"\n","        self.regnet.stem = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n","            nn.ReLU(inplace=True),\n","        )\n","        \n","        # Fine-tune the regnet classifier\n","        self.regnet.fc = nn.Sequential(\n","            nn.Linear(self.regnet.fc.in_features, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.PReLU(),\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.PReLU(),\n","            nn.Linear(512, num_classes),\n","        )\n","\n","        self.softmax = nn.Softmax(dim=1)\n"," \n","    def forward(self, x):\n","        logits = self.regnet(x)\n","        probas = self.softmax(logits)\n","\n","        return logits, probas\n","\n","\n","def train(model, train_loader, valid_loader, loss_func, optimizer, num_epochs, validate_on_train, scheduler, device):\n","    minibatch_loss, train_acc_lst, valid_acc_lst, train_loss_lst, valid_loss_lst = [], [], [], [], []\n","    \n","    for epoch in range(num_epochs):\n","        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n","        model.train()\n","        \n","        # use tqdm to show progress bar\n","        for row_id, audio_tensor, primary_label in tqdm(train_loader, total=len(train_loader), desc=\"Training batches\"):\n","\n","            features = audio_tensor.to(device)\n","            targets = primary_label.to(device)\n","\n","            logits, probas = model(features)\n","\n","            loss = loss_func(logits, targets)\n","\n","            optimizer.zero_grad()\n","            \n","            loss.backward()\n","\n","            minibatch_loss.append(loss.item())\n","            \n","            optimizer.step()\n","            \n","        train_acc, train_loss = validate(model, train_loader, loss_func) if validate_on_train else (torch.tensor(0.0), torch.tensor(0.0))\n","        train_acc_lst.append(train_acc)\n","        train_loss_lst.append(train_loss)\n","\n","        valid_acc, valid_loss = validate(model, valid_loader, loss_func)\n","        valid_acc_lst.append(valid_acc)\n","        valid_loss_lst.append(valid_loss)\n","\n","        if scheduler is not None:\n","            scheduler.step(valid_loss)\n","\n","        print(f\"Finsished epoch {epoch+1}/{num_epochs}. Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Valid Accuracy: {valid_acc:.2f}%\")\n","              \n","    return minibatch_loss, train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst\n","\n","\n","def validate(model, data_loader, loss_fn=F.cross_entropy):\n","    model.eval()\n","    \n","    num_examples, correct_pred, cross_entropy = 0.0, 0.0, 0.0\n","\n","    with torch.no_grad():\n","        for row_id, audio_tensor, primary_label in tqdm(data_loader, total=len(data_loader), desc='Validation batches'):\n","            features = audio_tensor.to(device)\n","            targets = primary_label.to(device)\n","\n","            logits, probas = model(features)\n","            cross_entropy += loss_fn(logits, targets)\n","\n","            _, predicted_labels = torch.max(probas, 1)\n","            num_examples += targets.size(0)\n","\n","            correct_pred += (predicted_labels == targets).sum()\n","\n","    accuracy = correct_pred / num_examples * 100\n","    loss = cross_entropy / num_examples\n","    return accuracy, loss\n","\n","\n","# --- training\n","ignore_existing_label_encoder = True\n","if ignore_existing_label_encoder or not os.path.exists('label_encoder.joblib'):\n","    print('Creating label encoder...')\n","    label_encoder = LabelEncoder()\n","    label_encoder.fit(list(unique_classes))\n","    joblib.dump(label_encoder, 'label_encoder.joblib')\n","else:\n","    print('Loading label encoder...')\n","    label_encoder = joblib.load('label_encoder.joblib')\n","\n","train_df, valid_df, test_df, class_weights = split_df(df_metadata_csv)\n","\n","seconds = 20 # 24 is the median - but 20 has better results\n","batch_size = 8\n","data_percentage = .5 # 1 means 100% of the data\n","num_epochs = 2\n","n_mels = 128 # 128 is the default value in librosa\n","learning_rate = 0.00008\n","# device = 'cpu'\n","pad_method = 'reflect' # wrap, constant_zero, reflect, wrap_double_reflect\n","\n","pin_memory = True # https://discuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723/6\n","validate_on_train = False\n","\n","train_dataset = BirdClef23Dataset(train_df, audio_data_dir, label_encoder, seconds, n_mels, device, pad_method=pad_method)\n","valid_dataset = BirdClef23Dataset(valid_df, audio_data_dir, label_encoder, seconds, n_mels, device, pad_method=pad_method)\n","# test_dataset = BirdClef23Dataset(test_df, audio_data_dir, label_encoder, seconds, n_mels, device, pad_method=pad_method)\n","\n","train_loader = get_data_loader(train_dataset, batch_size, data_percentage, shuffle=True, pin_memory=pin_memory)\n","valid_loader = get_data_loader(valid_dataset, batch_size, data_percentage, shuffle=False, pin_memory=pin_memory)\n","# test_loader = get_data_loader(test_dataset, batch_size, data_percentage, shuffle=False, pin_memory=pin_memory)\n","\n","model = RegnetCNN(num_classes=len(unique_classes)).to(device)\n","print(f\"Initialized model {model._get_name()}, trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n","# print(model)\n","\n","loss_function = torch.nn.CrossEntropyLoss(weight=torch.from_numpy(class_weights).float().to(device))\n","# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001) # worse accuracy\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n","scheduler = None\n","\n","print(f\"Seconds: {seconds}, batch_size: {batch_size}, data_percentage: {data_percentage}, num_epochs: {num_epochs}, n_mels: {n_mels}, learning_rate: {learning_rate}, pin_memory: {pin_memory}, validate_on_train: {validate_on_train}, device: {device}, pad_method: {pad_method}\")\n","\n","minibatch_loss, train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst = train(model, train_loader, valid_loader, loss_function, optimizer, num_epochs, validate_on_train, scheduler, device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utils.plot_minibatch_loss(minibatch_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utils.plot_train_and_valid_loss_and_accuracy(train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
