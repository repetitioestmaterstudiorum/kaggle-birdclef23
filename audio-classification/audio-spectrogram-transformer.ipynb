{
	"cells": [
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Fine-tuning an audio spectrogram transformer (AST) classifier on audio files (converted to spectrograms)\n",
				"\n",
				"AST is one of the best audio classification techniques today: https://paperswithcode.com/sota/audio-classification-on-audioset\n",
				"\n",
				"**Paper abstract:**\n",
				"\n",
				"In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2.\n",
				"\n",
				"https://arxiv.org/abs/2104.01778\n",
				"\n",
				"**Results**: ~0.3% accuracy\n",
				"\n",
				"**Notes**:\n",
				"- Had to use 16000 Hz sample rate for the audio files to be compatible with the AST feature extractor\n",
				"- 41min training time even with much reduced data: max. 3 seconds of audio per file (truncated), 5% of total samples only, train on the GPU (the model is quite large: 86'594'063 parameters)\n",
				"- More data and much more training time would be required for better results\n",
				"\n",
				"**Conclusion**: Not the model for this competition (requirement: maximum 2h training on CPU)\n",
				"\n",
				"**Next**: Research ..."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import pandas as pd\n",
				"import numpy as np\n",
				"import joblib\n",
				"import seaborn as sns\n",
				"import matplotlib.pyplot as plt\n",
				"from sklearn.model_selection import train_test_split\n",
				"import torch\n",
				"import torch.nn as nn\n",
				"import torch.optim as optim\n",
				"from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
				"from torch.utils.data import Dataset, DataLoader, random_split\n",
				"from torchvision.models import resnet18, ResNet18_Weights\n",
				"from torchvision import transforms\n",
				"from IPython.display import Audio\n",
				"import librosa\n",
				"from sklearn.preprocessing import LabelEncoder\n",
				"from sklearn.utils.class_weight import compute_class_weight\n",
				"from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
				"import torch.nn.functional as F\n",
				"from transformers import ASTFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer\n",
				"import torchaudio \n",
				"\n",
				"import random\n",
				"import glob\n",
				"import os\n",
				"import time\n",
				"\n",
				"import sys\n",
				"sys.path.append(\"..\")\n",
				"import utils"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"RANDOM_SEED = 21\n",
				"\n",
				"# Set seed for experiment reproducibility\n",
				"random.seed(RANDOM_SEED)\n",
				"np.random.seed(RANDOM_SEED)\n",
				"torch.manual_seed(RANDOM_SEED)\n",
				"torch.cuda.manual_seed(RANDOM_SEED)\n",
				"torch.backends.cudnn.deterministic = True\n",
				"torch.backends.cudnn.benchmark = True"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"is_in_kaggle_env = utils.get_is_in_kaggle_env()\n",
				"\n",
				"data_path = '/kaggle/input/birdclef-2023' if is_in_kaggle_env else '../data'\n",
				"\n",
				"device = 'cpu' if is_in_kaggle_env else utils.determine_device()\n",
				"\n",
				"if not is_in_kaggle_env and not os.path.exists('../data'):\n",
				"    print(\"Downloading data...\")\n",
				"    !kaggle competitions download -c 'birdclef-2023'\n",
				"    !mkdir ../data\n",
				"    !unzip -q birdclef-2023.zip -d ../data\n",
				"    !rm birdclef-2023.zip\n",
				"\n",
				"df_metadata_csv = pd.read_csv(f\"{data_path}/train_metadata.csv\")\n",
				"\n",
				"audio_data_dir = f\"{data_path}/train_audio/\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class_counts = df_metadata_csv[\"primary_label\"].value_counts()\n",
				"\n",
				"two_or_less_samples_rows = df_metadata_csv[df_metadata_csv[\"primary_label\"].isin(class_counts[class_counts < 3].index)]\n",
				"\n",
				"print(f\"Number of unique classes with less than 2 samples: {len(two_or_less_samples_rows['primary_label'].unique())}\")\n",
				"print(f\"Number of rows with less than 2 samples: {len(two_or_less_samples_rows)}\")\n",
				"print(f\"Primary labels with less than 2 samples: {two_or_less_samples_rows['primary_label'].unique()}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Drop rows with primary_label that have two or less samples\n",
				"print(f\"Number of rows before dropping: {len(df_metadata_csv)}\")\n",
				"df_metadata_csv = df_metadata_csv[~df_metadata_csv[\"primary_label\"].isin(class_counts[class_counts < 3].index)]\n",
				"print(f\"Number of rows after dropping: {len(df_metadata_csv)}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"unique_classes = df_metadata_csv.primary_label.unique()\n",
				"print(f\"Number of classes: {len(unique_classes)}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
				"_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
				"execution": {
					"iopub.execute_input": "2023-03-29T06:38:42.021281Z",
					"iopub.status.busy": "2023-03-29T06:38:42.019762Z",
					"iopub.status.idle": "2023-03-29T06:38:42.028449Z",
					"shell.execute_reply": "2023-03-29T06:38:42.027144Z",
					"shell.execute_reply.started": "2023-03-29T06:38:42.021210Z"
				},
				"trusted": true
			},
			"outputs": [],
			"source": [
				"class BirdClef23Dataset(Dataset):\n",
				"    def __init__(self, df, audio_data_dir, label_encoder, feature_extractor, seconds=10):\n",
				"        self.df = df\n",
				"        self.audio_data_dir = audio_data_dir\n",
				"        self.label_encoder = label_encoder\n",
				"        self.feature_extractor = feature_extractor\n",
				"        self.seconds = seconds\n",
				"\n",
				"    def __getitem__(self, index):\n",
				"        audio_path = os.path.join(self.audio_data_dir, self.df.iloc[index, 11])\n",
				"        audio_numpy, audio_sr = librosa.load(audio_path, sr=16000)\n",
				"\n",
				"        # Truncate the audio file to {seconds}\n",
				"        if len(audio_numpy) > audio_sr * self.seconds:\n",
				"            audio_numpy = audio_numpy[:audio_sr * self.seconds]\n",
				"\n",
				"        # Use the feature extractor to process the audio_numpy\n",
				"        inputs = self.feature_extractor(\n",
				"            audio_numpy,\n",
				"            sampling_rate=audio_sr,\n",
				"            max_length=audio_sr * self.seconds,\n",
				"            padding=\"max-length\",\n",
				"            return_tensors=\"pt\"\n",
				"        )\n",
				"        input_values = inputs.input_values[0]\n",
				"\n",
				"        primary_label_raw = self.df.iloc[index, 0]\n",
				"        primary_label = self.label_encoder.transform([primary_label_raw])[0]\n",
				"\n",
				"        row_id = audio_path.split('/')[-1].split('.')[0]\n",
				"\n",
				"        return {\"row_id\": row_id, \"input_values\": input_values, \"labels\": primary_label}\n",
				"\n",
				"    def __len__(self):\n",
				"        return len(self.df)\n",
				"\n",
				"\n",
				"def split_df(df, primary_label='primary_label', percentages=[60, 20, 20]):\n",
				"    \"\"\"\n",
				"    - Percentages: [train, valid, test]\n",
				"    - Splits a dataframe into three dataframes (train, valid, test), stratified by primary_label\n",
				"    - Also returns the class weights (based on the training set)\n",
				"    \"\"\"\n",
				"    print(f\"Splitting dataframe into train {percentages[0]}%, valid {percentages[1]}%, test {percentages[2]}%, stratified by {primary_label}\")\n",
				"    \n",
				"    train_perc, valid_perc, test_perc = [perc / 100 for perc in percentages]\n",
				"    train_valid_split = round(train_perc / (train_perc + valid_perc), 2)\n",
				"    \n",
				"    temp_df, test_df = train_test_split(df, test_size=test_perc, stratify=df[primary_label], random_state=RANDOM_SEED)\n",
				"    \n",
				"    train_df, valid_df = train_test_split(temp_df, test_size=1-train_valid_split, stratify=temp_df[primary_label], random_state=RANDOM_SEED)\n",
				"\n",
				"    classes = np.unique(train_df[primary_label])\n",
				"    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df[primary_label])\n",
				"\n",
				"    return train_df, valid_df, test_df, class_weights\n",
				"\n",
				"\n",
				"# --- training\n",
				"ignore_existing_label_encoder = True\n",
				"if ignore_existing_label_encoder or not os.path.exists('label_encoder.joblib'):\n",
				"    print('Creating label encoder...')\n",
				"    label_encoder = LabelEncoder()\n",
				"    label_encoder.fit(list(unique_classes))\n",
				"    joblib.dump(label_encoder, 'label_encoder.joblib')\n",
				"else:\n",
				"    print('Loading label encoder...')\n",
				"    label_encoder = joblib.load('label_encoder.joblib')\n",
				"\n",
				"data_percentage = 5\n",
				"seconds = 3\n",
				"batch_size = 16\n",
				"num_epochs = 1\n",
				"learning_rate = 0.0005\n",
				"\n",
				"feature_extractor = ASTFeatureExtractor(\n",
				"    sampling_rate=16000,\n",
				")\n",
				"\n",
				"train_df, valid_df, test_df, class_weights = split_df(df_metadata_csv)\n",
				"\n",
				"print(f\"Using {data_percentage}% of the data\")\n",
				"train_df = train_df.sample(frac=data_percentage/100, random_state=RANDOM_SEED)\n",
				"valid_df = valid_df.sample(frac=data_percentage/100, random_state=RANDOM_SEED)\n",
				"test_df = test_df.sample(frac=data_percentage/100, random_state=RANDOM_SEED)\n",
				"\n",
				"train_dataset = BirdClef23Dataset(train_df, audio_data_dir, label_encoder, feature_extractor, seconds)\n",
				"valid_dataset = BirdClef23Dataset(valid_df, audio_data_dir, label_encoder, feature_extractor, seconds)\n",
				"test_dataset = BirdClef23Dataset(test_df, audio_data_dir, label_encoder, feature_extractor, seconds)\n",
				"\n",
				"model = AutoModelForAudioClassification.from_pretrained(\n",
				"    \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
				")\n",
				"print(f\"Initialized model {model._get_name()}\")\n",
				"\n",
				"# MPS Training class https://github.com/huggingface/transformers/issues/17971\n",
				"class TrainingArgumentsWithMPSSupport(TrainingArguments):\n",
				"    @property\n",
				"    def device(self) -> torch.device:\n",
				"        return torch.device(device)\n",
				"\n",
				"training_args = TrainingArgumentsWithMPSSupport(\n",
				"    output_dir=\"birdclef-2023-ast\",\n",
				"    evaluation_strategy=\"epoch\",\n",
				"    save_strategy=\"epoch\",\n",
				"    learning_rate=learning_rate,\n",
				"    per_device_train_batch_size=batch_size,\n",
				"    gradient_accumulation_steps=4,\n",
				"    per_device_eval_batch_size=batch_size,\n",
				"    num_train_epochs=num_epochs,\n",
				"    warmup_ratio=0.1,\n",
				"    logging_steps=10,\n",
				"    load_best_model_at_end=True,\n",
				"    metric_for_best_model=\"accuracy\",\n",
				"    push_to_hub=False,\n",
				")\n",
				"\n",
				"def custom_metrics_fn(eval_pred):\n",
				"    logits, labels = eval_pred\n",
				"    predictions = np.argmax(logits, axis=-1)\n",
				"    return {\n",
				"        \"accuracy\": (predictions == labels).mean().item()\n",
				"    }\n",
				"\n",
				"trainer = Trainer(\n",
				"    model=model.to(device),\n",
				"    args=training_args,\n",
				"    train_dataset=train_dataset,\n",
				"    eval_dataset=valid_dataset,\n",
				"    tokenizer=feature_extractor,\n",
				"    compute_metrics=custom_metrics_fn,\n",
				")\n",
				"\n",
				"trainer.train()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# print the amount of parameters in the model\n",
				"print(f\"Model has {trainer.model.num_parameters()} parameters\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.10.10"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
