{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## RegNet Training"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["import joblib\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision.models import regnet_y_1_6gf, RegNet_Y_1_6GF_Weights\n","from torchvision import transforms\n","from IPython.display import Audio\n","import librosa\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n","import torch.nn.functional as F\n","from torchaudio import functional as F_audio\n","from tqdm import tqdm\n","import timm\n","import torchaudio\n","\n","import copy\n","import random\n","import glob\n","import os\n","import time\n","import sys\n","import re\n","import pywt"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["RANDOM_SEED = 21\n","\n","# Set seed for experiment reproducibility\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","torch.cuda.manual_seed(RANDOM_SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We are running code on Localhost\n","We are using device: mps\n"]}],"source":["## REUSE IN INFERENCE NOTEBOOK\n","\n","custom_dataset_path = '/kaggle/input/birdclef2023-inference'\n","if os.path.exists(os.path.join(custom_dataset_path, 'utils.py')):\n","    sys.path.append(custom_dataset_path)\n","else:\n","    sys.path.append('..')\n","import utils\n","\n","IS_IN_KAGGLE_ENV = utils.get_is_in_kaggle_env()\n","\n","DATA_PATH = '/kaggle/input/birdclef-2023' if IS_IN_KAGGLE_ENV else '../data'\n","JOBLIB_PATH = custom_dataset_path if IS_IN_KAGGLE_ENV else './'\n","\n","DEVICE = 'cpu' if IS_IN_KAGGLE_ENV else utils.determine_device()\n","\n","AUDIO_LENGTH_S = 5\n","SAMPLE_RATE = 32_000"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["if not IS_IN_KAGGLE_ENV and not os.path.exists(DATA_PATH):\n","    print(\"Downloading data ...\")\n","    !kaggle competitions download -c 'birdclef-2023'\n","    !mkdir ../data\n","    !unzip -q birdclef-2023.zip -d ../data\n","    !rm birdclef-2023.zip\n","\n","DF_METADATA_CSV = pd.read_csv(f\"{DATA_PATH}/train_metadata.csv\")\n","\n","AUDIO_DATA_DIR = f\"{DATA_PATH}/train_audio/\""]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of rows with 1 sample: 7\n","Number of rows with 2 samples: 6\n","Number of rows with 1 sample: 0\n","Number of rows with 2 samples: 0\n"]}],"source":["# Rows with 1 sample: copy twice, rows with 2 samples: copy once\n","# This is to ensure that stratified sampling for train/valid/test splits works correctly\n","def ensure_min_two_samples(DF_METADATA_CSV):\n","    class_counts = DF_METADATA_CSV[\"primary_label\"].value_counts()\n","\n","    one_sample_rows = DF_METADATA_CSV[DF_METADATA_CSV[\"primary_label\"].isin(class_counts[class_counts == 1].index)]\n","    print(f\"Number of rows with 1 sample: {len(one_sample_rows)}\")\n","    if len(one_sample_rows) > 0:\n","        DF_METADATA_CSV = pd.concat([DF_METADATA_CSV, one_sample_rows, one_sample_rows], ignore_index=True)\n","    \n","    two_sample_rows = DF_METADATA_CSV[DF_METADATA_CSV[\"primary_label\"].isin(class_counts[class_counts == 2].index)]\n","    print(f\"Number of rows with 2 samples: {len(two_sample_rows)}\")\n","    if len(two_sample_rows) > 0:\n","        DF_METADATA_CSV = pd.concat([DF_METADATA_CSV, two_sample_rows], ignore_index=True)\n","\n","    return DF_METADATA_CSV\n","\n","DF_METADATA_CSV = ensure_min_two_samples(DF_METADATA_CSV)\n","\n","# Run again to verify\n","DF_METADATA_CSV = ensure_min_two_samples(DF_METADATA_CSV)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["## REUSE IN INFERENCE NOTEBOOK\n","\n","class BirdMelspecClf(nn.Module):\n","    def __init__(self, out_features, pretrained):\n","        super().__init__()\n","        \n","        # https://pytorch.org/vision/stable/models.html\n","        self.regnet = regnet_y_1_6gf(weights=RegNet_Y_1_6GF_Weights.DEFAULT) if pretrained else regnet_y_1_6gf()\n","\n","        \"\"\"\n","        Replace the stem to take 2 channels instead of 3. The original stem:\n","        (stem): SimpleStemIN(\n","            (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (2): ReLU(inplace=True)\n","            )\n","        )\"\"\"\n","        self.regnet.stem[0] = nn.Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        \n","        # Replace original classifier: (fc): Linear(in_features=888, out_features=1000, bias=True)\n","        self.regnet.fc = nn.Linear(self.regnet.fc.in_features, out_features)\n","\n","        self.softmax = nn.Softmax(dim=1)\n"," \n","    def forward(self, x):\n","        logits = self.regnet(x)\n","        probas = self.softmax(logits)\n","\n","        return logits, probas\n","\n","\n","def get_model(out_features, device, pretrained=False, load_state_dict=True, state_dict_starts_with=f\"{AUDIO_LENGTH_S}s_regnetY800MF\"):\n","    model = BirdMelspecClf(out_features=out_features, pretrained=pretrained)\n","    print(f\"Loaded model {model.__class__.__name__} with {sum(p.numel() for p in model.parameters())} parameters, pretained={pretrained}\")\n","    model.to(device)\n","\n","    if not load_state_dict:\n","        return model\n","\n","    model_files = [f for f in os.listdir(JOBLIB_PATH) if f.startswith(state_dict_starts_with) and f.endswith('.pt')]\n","    if len(model_files) == 0:\n","        print(f\"No model starting with {state_dict_starts_with} found in {JOBLIB_PATH}\")\n","        return model\n","    \n","    # Extract timestamp from the filenames and sort based on it\n","    model_files.sort(key=lambda x: int(re.findall(r'\\d+', x)[-1]) if re.findall(r'\\d+', x) else -1)\n","\n","    # The latest model file is the last one in the sorted list\n","    latest_model_file = model_files[-1]\n","    model_path = os.path.join(JOBLIB_PATH, latest_model_file)\n","    model.load_state_dict(torch.load(model_path))\n","    print(f\"Loaded model weights from {model_path}\")\n","    model.to(device)\n","\n","    return model"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of classes: 264\n"]}],"source":["UNIQUE_CLASSES = DF_METADATA_CSV.primary_label.unique()\n","N_CLASSES = len(UNIQUE_CLASSES)\n","print(f\"Number of classes: {N_CLASSES}\")"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["(264, '../data/train_audio/ruegls1')"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["train_classes_paths = glob.glob(f\"{DATA_PATH}/train_audio/*\")\n","len(train_classes_paths), train_classes_paths[0]"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total samples in dataset: 16961\n"]}],"source":["print(f\"Total samples in dataset: {len(DF_METADATA_CSV)}\")"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Class with fewest samples: crefra2: 3 samples\n"]}],"source":["class_with_fewest_samples = DF_METADATA_CSV.primary_label.value_counts(sort=True).index[-1]\n","print(f\"Class with fewest samples: {class_with_fewest_samples}: {DF_METADATA_CSV.primary_label.value_counts().min()} samples\")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["class WaveletTransformSingle(nn.Module):\n","  def __init__(\n","      self, \n","      wavelet: pywt.Wavelet,\n","      cut_to_nearest: int | None = None\n","  ):\n","    super(WaveletTransformSingle, self).__init__()\n","    self.wavelet = wavelet\n","    self.ctn = cut_to_nearest\n","\n","  def forward(self, X: torch.Tensor) -> torch.Tensor:\n","    item = X.cpu().numpy()\n","    \n","    wh, wl = pywt.dwt(item[0], self.wavelet)\n","    out = torch.stack((torch.from_numpy(wh), torch.from_numpy(wl)))\n","    \n","    if self.ctn is not None:\n","      out = out[:,:-1 * (out.shape[-1] % self.ctn)]\n","    \n","    return out"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["## REUSE IN INFERENCE NOTEBOOK\n","\n","def resample(audio, current_sample_rate, desired_sample_rate=SAMPLE_RATE):\n","    resampler = torchaudio.transforms.Resample(orig_freq=current_sample_rate, new_freq=desired_sample_rate)\n","    resampled_audio = resampler(audio)\n","    return resampled_audio\n","\n","def load_audio(audio_path, sample_rate=SAMPLE_RATE):\n","    audio, sr = torchaudio.load(audio_path)\n","    if sr != sample_rate:\n","        audio = resample(audio, sr, sample_rate)\n","    return audio\n","\n","# Using librosa defaults for n_fft and hop_length\n","def get_melspec_transform(sample_rate=SAMPLE_RATE, n_fft=2048, hop_length=512, n_mels=128):\n","    return torchaudio.transforms.MelSpectrogram(\n","        sample_rate=sample_rate,\n","        n_fft=n_fft,\n","        hop_length=hop_length,\n","        n_mels=n_mels,\n","    )\n","\n","# Using librosa defaults for top_db\n","def get_melspec_db_transform(stype='power', top_db=80):\n","    return torchaudio.transforms.AmplitudeToDB(\n","        stype=stype,\n","        top_db=top_db\n","    )\n","\n","# Copied from torchaudio/transforms/_transforms.py (to avoid converting to melspec twice)\n","dct_mat = F_audio.create_dct(128, 128, \"ortho\")\n","def get_mfcc_from_melspec(melspec):\n","    return torch.matmul(melspec.transpose(-1, -2), dct_mat).transpose(-1, -2)\n","\n","def normalize_tensor(tensor):\n","    min_val = torch.min(tensor)\n","    max_val = torch.max(tensor)\n","    if max_val - min_val == 0:\n","        return tensor\n","    else:\n","        return (tensor - min_val) / (max_val - min_val)"]},{"cell_type":"code","execution_count":26,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-03-29T06:38:42.021281Z","iopub.status.busy":"2023-03-29T06:38:42.019762Z","iopub.status.idle":"2023-03-29T06:38:42.028449Z","shell.execute_reply":"2023-03-29T06:38:42.027144Z","shell.execute_reply.started":"2023-03-29T06:38:42.021210Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating label encoder...\n","Splitting dataframe into train 0.8%, valid 0.2%, test 0%, stratified by primary_label\n","Loaded model BirdMelspecClf with 10548414 parameters, pretained=True\n","batch_size: 8, data_percentage: 1, num_epochs: 20, n_mels: 128, learning_rate: 0.0001, pin_memory: True, \n","validate_on_train: True, device: mps, pad_method: wrap, validate_train_pct: 0.2\n"]},{"name":"stderr","output_type":"stream","text":["Validation batches: 100%|██████████| 425/425 [02:03<00:00,  3.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Initial validation accuracy: 0.0884%\n","Starting epoch 1/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|██████████| 1696/1696 [14:35<00:00,  1.94it/s]\n","Validation batches: 100%|██████████| 339/339 [01:38<00:00,  3.45it/s]\n","Validation batches: 100%|██████████| 425/425 [02:04<00:00,  3.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_regnetY16GF_e0_valacc5_traacc7_noaug_1684830595000.pt ...\n","Finsished epoch 1/20. Train Loss: 0.6664, Valid Loss: 0.6711, Train Accuracy: 6.53%, Valid Accuracy: 5.07%\n","Starting epoch 2/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches: 100%|██████████| 1696/1696 [14:39<00:00,  1.93it/s]\n","Validation batches: 100%|██████████| 339/339 [01:34<00:00,  3.59it/s]\n","Validation batches: 100%|██████████| 425/425 [02:01<00:00,  3.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Moving model to CPU and saving it as 5s_regnetY16GF_e1_valacc18_traacc19_noaug_1684830595001.pt ...\n","Finsished epoch 2/20. Train Loss: 0.5589, Valid Loss: 0.5836, Train Accuracy: 18.58%, Valid Accuracy: 17.77%\n","Starting epoch 3/20\n"]},{"name":"stderr","output_type":"stream","text":["Training batches:  56%|█████▋    | 957/1696 [08:24<06:07,  2.01it/s]"]}],"source":["class BirdClef23Dataset(Dataset):\n","    def __init__(self, df, audio_data_dir, label_encoder, n_mels, device, pad_method='wrap'):\n","        self.df = df\n","        self.audio_data_dir = audio_data_dir\n","        self.label_encoder = label_encoder\n","        self.device = device\n","        self.pad_method = pad_method\n","        self.melspec_transform = get_melspec_transform(n_mels=n_mels)\n","        self.melspec_db_transform = get_melspec_db_transform()\n","        self.wave_transform = WaveletTransformSingle(pywt.Wavelet('sym4'))\n","        self.resize = transforms.Resize((128, 313), antialias=True)\n","\n","    def __getitem__(self, index):\n","        audio_path = os.path.join(self.audio_data_dir, self.df.iloc[index, 11])\n","        audio = load_audio(audio_path, SAMPLE_RATE)\n","\n","        # Increase audio length if below {AUDIO_LENGTH_S} by padding\n","        if audio.shape[1] < AUDIO_LENGTH_S * SAMPLE_RATE:\n","            padding_needed = AUDIO_LENGTH_S * SAMPLE_RATE - audio.shape[1]\n","\n","            if self.pad_method == 'wrap':\n","                audio = F.pad(audio, (0, padding_needed), mode='replicate')\n","\n","            elif self.pad_method == 'zeros':\n","                audio = F.pad(audio, (0, padding_needed), mode='constant', value=0)\n","\n","        # Truncate audio length if above {AUDIO_LENGTH_S} by random cropping\n","        if audio.shape[1] > AUDIO_LENGTH_S * SAMPLE_RATE:\n","            max_start_idx = audio.shape[1] - (SAMPLE_RATE * AUDIO_LENGTH_S)\n","            start_idx = torch.randint(0, max_start_idx, (1,)).item()\n","            audio = audio[:, start_idx:start_idx + (SAMPLE_RATE * AUDIO_LENGTH_S)]\n","\n","        melspec = self.melspec_db_transform(self.melspec_transform(audio))\n","        norm_melspec = normalize_tensor(melspec)\n","        melspec_wave = self.wave_transform(audio)\n","        wh, wl = melspec_wave[0], melspec_wave[1]\n","        wh_mel = self.melspec_db_transform(self.melspec_transform(wh))\n","        wl_mel = self.melspec_db_transform(self.melspec_transform(wl))\n","        norm_wh = normalize_tensor(self.resize(wh_mel.unsqueeze(0)))\n","        norm_wl = normalize_tensor(self.resize(wl_mel.unsqueeze(0)))\n","\n","        mfcc = get_mfcc_from_melspec(melspec)\n","        norm_mfcc = normalize_tensor(self.resize(mfcc))\n","\n","        features = torch.cat((norm_melspec, norm_wh, norm_wl, norm_mfcc), dim=0)\n","        \n","        primary_label_raw = self.df.iloc[index, 0]\n","        primary_label = self.label_encoder.transform([primary_label_raw])[0]\n","\n","        return features, primary_label\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","\n","def get_data_loader(dataset, batch_size=32, data_percentage=None, shuffle=False, pin_memory=False):\n","    if data_percentage is not None:\n","        data_len = int(len(dataset) * data_percentage)\n","        dataset, _ = random_split(dataset, [data_len, len(dataset) - data_len])\n","\n","    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n","\n","    return data_loader\n","\n","\n","def split_df(df, primary_label='primary_label', percentages=[60, 20, 20]):\n","    \"\"\"\n","    - Percentages: [train, valid, test]\n","    - Splits a dataframe into three dataframes (train, valid, test), stratified by primary_label\n","    - Also returns the class weights (based on the training set)\n","    \"\"\"\n","    print(f\"Splitting dataframe into train {percentages[0]}%, valid {percentages[1]}%, test {percentages[2]}%, stratified by {primary_label}\")\n","    \n","    train_perc, valid_perc, test_perc = [perc / 100 for perc in percentages]\n","    train_valid_split = round(train_perc / (train_perc + valid_perc), 2)\n","    \n","    if test_perc == 0:\n","        temp_df = df\n","        test_df = None\n","    else:\n","        temp_df, test_df = train_test_split(df, test_size=test_perc, stratify=df[primary_label], random_state=RANDOM_SEED)\n","    \n","    train_df, valid_df = train_test_split(temp_df, test_size=1-train_valid_split, stratify=temp_df[primary_label], random_state=RANDOM_SEED)\n","\n","    classes = np.unique(train_df[primary_label])\n","    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df[primary_label])\n","\n","    return train_df, valid_df, test_df, class_weights\n","\n","\n","def convert_to_three_digits(num):\n","    if num < 100:\n","        return str(num).zfill(3)\n","    else:\n","        return str(num)\n","    \n","\n","def train(model, train_loader, valid_loader, loss_func, optimizer, num_epochs, validate_on_train, validate_train_pct, scheduler, device, augmentations=None):\n","    minibatch_loss, train_acc_lst, valid_acc_lst, train_loss_lst, valid_loss_lst, best_valid_acc = [], [], [], [], [], 0.0\n","    train_start_ts = str(time.time()).split('.')[0]\n","\n","    # Initial validation step for models with loaded weights, so new weights are only saved if they improve on the loaded ones\n","    valid_acc, valid_loss = validate(model, device, valid_loader, loss_func)\n","    best_valid_acc = valid_acc\n","    print(f\"Initial validation accuracy: {valid_acc:.4f}%\")\n","    \n","    for epoch in range(num_epochs):\n","        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n","        model.train()\n","        \n","        for melspec, primary_label in tqdm(train_loader, total=len(train_loader), desc=\"Training batches\"):\n","            features = melspec.to(device)\n","            targets = primary_label.to(device=device, dtype=torch.long)\n","\n","\n","            logits, probas = model(features)\n","\n","            loss = loss_func(logits, targets)\n","\n","            optimizer.zero_grad()\n","            \n","            loss.backward()\n","\n","            minibatch_loss.append(loss.item())\n","            \n","            optimizer.step()\n","        \n","        train_acc, train_loss = validate(model, device, train_loader, loss_func, validate_train_pct) if validate_on_train else (torch.tensor(0.0), torch.tensor(0.0))\n","        valid_acc, valid_loss = validate(model, device, valid_loader, loss_func)\n","\n","        train_acc_lst.append(train_acc)\n","        train_loss_lst.append(train_loss)\n","        valid_acc_lst.append(valid_acc)\n","        valid_loss_lst.append(valid_loss)\n","\n","        if valid_acc > best_valid_acc:\n","            best_valid_acc = valid_acc\n","            model_name = f\"{AUDIO_LENGTH_S}s_regnetY16GF_e{epoch}_valacc{valid_acc:.0f}_traacc{train_acc:.0f}_{'aug' if augmentations else 'noaug'}_{train_start_ts}{convert_to_three_digits(epoch)}.pt\"\n","            print(f\"Moving model to CPU and saving it as {model_name} ...\")\n","            model_cpu = copy.deepcopy(model)\n","            model_cpu.to('cpu')\n","            model_path = os.path.join(JOBLIB_PATH, model_name)\n","            torch.save(model_cpu.state_dict(), model_path)\n","\n","        if scheduler is not None:\n","            lr_before = optimizer.param_groups[0]['lr']\n","            scheduler.step(valid_loss)\n","            lr_after = optimizer.param_groups[0]['lr']\n","            if lr_before != lr_after:\n","                print(f\"Learning rate changed from {lr_before} to {lr_after}\")\n","\n","        print(f\"Finsished epoch {epoch+1}/{num_epochs}. Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Valid Accuracy: {valid_acc:.2f}%\")\n","              \n","    return minibatch_loss, train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst\n","\n","\n","def validate(model, device, data_loader, loss_fn=F.cross_entropy, data_percentage=None):\n","    model.eval()\n","    \n","    num_examples, correct_pred, cross_entropy = 0.0, 0.0, 0.0\n","\n","    with torch.no_grad():\n","        data_loader_len = len(data_loader) if data_percentage is None else int(len(data_loader) * data_percentage)\n","        i = 0\n","        for melspec, primary_label in tqdm(data_loader, total=data_loader_len, desc='Validation batches'):\n","            if data_percentage is not None and i >= data_loader_len:\n","                break\n","            i += 1\n","\n","            features = melspec.to(device)\n","            targets = primary_label.to(device=device, dtype=torch.long)\n","\n","            logits, probas = model(features)\n","            cross_entropy += loss_fn(logits, targets)\n","\n","            _, predicted_labels = torch.max(probas, 1)\n","\n","            num_examples += targets.size(0)\n","\n","            correct_pred += (predicted_labels == targets).sum()\n","\n","    accuracy = correct_pred / num_examples * 100\n","    loss = cross_entropy / num_examples\n","    return accuracy, loss\n","\n","\n","# --- training\n","print('Creating label encoder...')\n","label_encoder = LabelEncoder()\n","label_encoder.fit(list(UNIQUE_CLASSES))\n","joblib.dump(label_encoder, 'label_encoder.joblib')\n","\n","train_df, valid_df, test_df, class_weights = split_df(DF_METADATA_CSV, percentages=[0.8, 0.2, 0])\n","\n","batch_size = 8\n","data_percentage = 1 # 1 means 100% of the data\n","num_epochs = 20\n","n_mels = 128 # 128 is the default value in librosa\n","learning_rate = 0.0001\n","# DEVICE = 'cpu'\n","pad_method = 'wrap' # wrap, zeros\n","\n","# augmentation_transforms = nn.Sequential(\n","#     T_audio.TimeStretch(0.8, fixed_rate=True),\n","#     T_audio.FrequencyMasking(freq_mask_param=30),\n","#     T_audio.TimeMasking(time_mask_param=80),\n","# )\n","augmentation_transforms = None\n","pin_memory = True # https://discuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723/6\n","validate_on_train = True\n","validate_train_pct = 0.2\n","\n","train_dataset = BirdClef23Dataset(train_df, AUDIO_DATA_DIR, label_encoder, n_mels, DEVICE, pad_method=pad_method)\n","valid_dataset = BirdClef23Dataset(valid_df, AUDIO_DATA_DIR, label_encoder, n_mels, DEVICE, pad_method=pad_method)\n","\n","train_loader = get_data_loader(train_dataset, batch_size, data_percentage, shuffle=True, pin_memory=pin_memory)\n","valid_loader = get_data_loader(valid_dataset, batch_size, data_percentage, shuffle=False, pin_memory=pin_memory)\n","\n","# model = get_model(out_features=N_CLASSES, device=DEVICE, pretrained=True, load_state_dict=False)\n","model = get_model(out_features=N_CLASSES, device=DEVICE, pretrained=False, load_state_dict=True)\n","\n","loss_function = torch.nn.CrossEntropyLoss(weight=torch.from_numpy(class_weights).float().to(DEVICE))\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n","scheduler = None\n","\n","print(f\"batch_size: {batch_size}, data_percentage: {data_percentage}, num_epochs: {num_epochs}, n_mels: {n_mels}, learning_rate: {learning_rate}, pin_memory: {pin_memory}, \\nvalidate_on_train: {validate_on_train}, device: {DEVICE}, pad_method: {pad_method}, validate_train_pct: {validate_train_pct}\")\n","\n","minibatch_loss, train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst = train(model, train_loader, valid_loader, loss_function, optimizer, num_epochs, validate_on_train, validate_train_pct, scheduler, DEVICE, augmentation_transforms)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utils.plot_minibatch_loss(minibatch_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utils.plot_train_and_valid_loss_and_accuracy(train_loss_lst, valid_loss_lst, train_acc_lst, valid_acc_lst)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
