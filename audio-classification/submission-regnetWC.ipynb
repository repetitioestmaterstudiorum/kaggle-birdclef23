{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ebd60c0",
   "metadata": {},
   "source": [
    "# RegNet Inference (submission generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b594cdc3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-14T04:40:42.750183Z",
     "iopub.status.busy": "2023-04-14T04:40:42.749779Z",
     "iopub.status.idle": "2023-04-14T04:40:47.150440Z",
     "shell.execute_reply": "2023-04-14T04:40:47.149088Z"
    },
    "papermill": {
     "duration": 4.409497,
     "end_time": "2023-04-14T04:40:47.153201",
     "exception": false,
     "start_time": "2023-04-14T04:40:42.743704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torchvision.models import regnet_y_800mf, RegNet_Y_800MF_Weights\n",
    "import timm\n",
    "import re\n",
    "from torchaudio import functional as F_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be457b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REUSE IN INFERENCE NOTEBOOK\n",
    "\n",
    "custom_dataset_path = '/kaggle/input/birdclef2023-inference'\n",
    "if os.path.exists(os.path.join(custom_dataset_path, 'utils.py')):\n",
    "    sys.path.append(custom_dataset_path)\n",
    "else:\n",
    "    sys.path.append('..')\n",
    "import utils\n",
    "\n",
    "IS_IN_KAGGLE_ENV = utils.get_is_in_kaggle_env()\n",
    "\n",
    "DATA_PATH = '/kaggle/input/birdclef-2023' if IS_IN_KAGGLE_ENV else '../data'\n",
    "JOBLIB_PATH = custom_dataset_path if IS_IN_KAGGLE_ENV else './'\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "AUDIO_LENGTH_S = 5\n",
    "SAMPLE_RATE = 32_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a40c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T04:40:47.281054Z",
     "iopub.status.busy": "2023-04-14T04:40:47.280329Z",
     "iopub.status.idle": "2023-04-14T04:40:47.286543Z",
     "shell.execute_reply": "2023-04-14T04:40:47.285263Z"
    },
    "papermill": {
     "duration": 0.013263,
     "end_time": "2023-04-14T04:40:47.289034",
     "exception": false,
     "start_time": "2023-04-14T04:40:47.275771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## REUSE IN INFERENCE NOTEBOOK\n",
    "\n",
    "class BirdMelspecClf(nn.Module):\n",
    "    def __init__(self, out_features, pretrained):\n",
    "        super().__init__()\n",
    "        \n",
    "        # https://pytorch.org/vision/stable/models.html\n",
    "\n",
    "        self.regnet = regnet_y_800mf(weights=RegNet_Y_800MF_Weights.DEFAULT) if pretrained else regnet_y_800mf()\n",
    "\n",
    "        \"\"\"\n",
    "        Replace the stem to take 1 channel instead of 3. The original stem:\n",
    "        RegnetCNN(\n",
    "        (regnet): RegNet(\n",
    "            (stem): SimpleStemIN(\n",
    "            (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (2): ReLU(inplace=True)\n",
    "        )\"\"\"\n",
    "        self.regnet.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Fine-tune the regnet classifier\n",
    "        self.regnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.regnet.fc.in_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.PReLU(),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.Linear(256, out_features),\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        logits = self.regnet(x)\n",
    "        probas = self.softmax(logits)\n",
    "\n",
    "        return logits, probas\n",
    "\n",
    "\n",
    "def get_model(out_features, device, pretrained=False, load_state_dict=True, state_dict_starts_with=f\"{AUDIO_LENGTH_S}s_regnetY800MFWC\"):\n",
    "    model = BirdMelspecClf(out_features=out_features, pretrained=pretrained)\n",
    "    print(f\"Loaded model {model.__class__.__name__} with {sum(p.numel() for p in model.parameters())} parameters, pretained={pretrained}\")\n",
    "    model.to(device)\n",
    "\n",
    "    if not load_state_dict:\n",
    "        return model\n",
    "\n",
    "    model_files = [f for f in os.listdir(JOBLIB_PATH) if f.startswith(state_dict_starts_with) and f.endswith('.pt')]\n",
    "    if len(model_files) == 0:\n",
    "        print(f\"No model starting with {state_dict_starts_with} found in {JOBLIB_PATH}\")\n",
    "        return model\n",
    "    \n",
    "    # Extract timestamp from the filenames and sort based on it\n",
    "    model_files.sort(key=lambda x: int(re.findall(r'\\d+', x)[-1]) if re.findall(r'\\d+', x) else -1)\n",
    "\n",
    "    # The latest model file is the last one in the sorted list\n",
    "    latest_model_file = model_files[-1]\n",
    "    model_path = os.path.join(JOBLIB_PATH, latest_model_file)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(f\"Loaded model weights from {model_path}\")\n",
    "    model.to(device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_label_encoder():\n",
    "    label_encoder_path = os.path.join(JOBLIB_PATH, 'label_encoder.joblib')\n",
    "    label_encoder = joblib.load(label_encoder_path)\n",
    "    print(f\"Loaded label encoder from {label_encoder_path}\")\n",
    "    return label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456150a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T04:40:47.167665Z",
     "iopub.status.busy": "2023-04-14T04:40:47.167337Z",
     "iopub.status.idle": "2023-04-14T04:40:47.263629Z",
     "shell.execute_reply": "2023-04-14T04:40:47.262092Z"
    },
    "papermill": {
     "duration": 0.103522,
     "end_time": "2023-04-14T04:40:47.266309",
     "exception": false,
     "start_time": "2023-04-14T04:40:47.162787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## REUSE IN INFERENCE NOTEBOOK\n",
    "\n",
    "def resample(audio, current_sample_rate, desired_sample_rate=SAMPLE_RATE):\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=current_sample_rate, new_freq=desired_sample_rate)\n",
    "    resampled_audio = resampler(audio)\n",
    "    return resampled_audio\n",
    "\n",
    "def load_audio(audio_path, sample_rate=SAMPLE_RATE):\n",
    "    audio, sr = torchaudio.load(audio_path)\n",
    "    if sr != sample_rate:\n",
    "        audio = resample(audio, sr, sample_rate)\n",
    "    return audio\n",
    "\n",
    "# Using librosa defaults for n_fft and hop_length\n",
    "def get_melspec_transform(sample_rate=SAMPLE_RATE, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    return torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "    )\n",
    "\n",
    "# Using librosa defaults for top_db\n",
    "def get_melspec_db_transform(stype='power', top_db=80):\n",
    "    return torchaudio.transforms.AmplitudeToDB(\n",
    "        stype=stype,\n",
    "        top_db=top_db\n",
    "    )\n",
    "\n",
    "# Copied from torchaudio/transforms/_transforms.py (to avoid converting to melspec twice)\n",
    "dct_mat = F_audio.create_dct(40, 128, \"ortho\")\n",
    "def get_mfcc_from_melspec(melspec):\n",
    "    return torch.matmul(melspec.transpose(-1, -2), dct_mat).transpose(-1, -2)\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    min_val = torch.min(tensor)\n",
    "    max_val = torch.max(tensor)\n",
    "    if max_val - min_val == 0:\n",
    "        return tensor\n",
    "    else:\n",
    "        return (tensor - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = glob.glob(f\"{DATA_PATH}/test_soundscapes/*.ogg\")\n",
    "print(f\"filepaths length: {len(filepaths)} (amount of test soundscapes)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75cb3717",
   "metadata": {
    "papermill": {
     "duration": 0.002934,
     "end_time": "2023-04-14T04:40:48.158297",
     "exception": false,
     "start_time": "2023-04-14T04:40:48.155363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca7fea8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T04:40:48.166858Z",
     "iopub.status.busy": "2023-04-14T04:40:48.165617Z",
     "iopub.status.idle": "2023-04-14T04:40:57.513978Z",
     "shell.execute_reply": "2023-04-14T04:40:57.512914Z"
    },
    "papermill": {
     "duration": 9.356216,
     "end_time": "2023-04-14T04:40:57.517571",
     "exception": false,
     "start_time": "2023-04-14T04:40:48.161355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "simulate_200_files = False\n",
    "\n",
    "if simulate_200_files:\n",
    "    filepaths = [filepaths[0] for i in range(200)] # simulate submission\n",
    "    print(f\"filepaths length: {len(filepaths)} after simulation additions\")\n",
    "\n",
    "label_encoder = get_label_encoder()\n",
    "model = get_model(out_features=len(label_encoder.classes_), device=DEVICE, pretrained=False, load_state_dict=True)\n",
    "model.eval()\n",
    "\n",
    "MIN_WINDOW = AUDIO_LENGTH_S * SAMPLE_RATE\n",
    "melspec_transform = get_melspec_transform(n_mels=128)\n",
    "melspec_db_transform = get_melspec_db_transform()\n",
    "\n",
    "def infer(filepath):\n",
    "    all_predictions = []\n",
    "    name = Path(filepath).stem\n",
    "    audio = load_audio(filepath)\n",
    "    audio_len_s = audio.shape[1] / SAMPLE_RATE\n",
    "    debug and print(f\"Infering file {filepath} with length {audio_len_s} s\")\n",
    "    n_crops = int(audio_len_s // 5)\n",
    "    for i in range(n_crops):\n",
    "        debug and print(f\"Crop {i} / {n_crops}\")\n",
    "        debug and print(f\"Audio length: {len(audio)}\")\n",
    "        crop = audio[:, i*MIN_WINDOW:(i+1)*MIN_WINDOW]\n",
    "        debug and print(f\"Crop dimensions: {crop.shape}\")\n",
    "        melspec = normalize_tensor(melspec_db_transform(melspec_transform(crop)))\n",
    "        debug and print(f\"melspec shape: {melspec.shape}\") # [1, 128, 313]\n",
    "        melspec = melspec.unsqueeze(0) # add batch dimension (1)\n",
    "        debug and print(f\"melspec unsqueezed shape: {melspec.shape}\") # [1, 1, 128, 313]\n",
    "        with torch.no_grad():\n",
    "            logit, proba = model(melspec)\n",
    "        t = (i + 1) * 5\n",
    "        all_predictions.append({\"row_id\": f'{name}_{t}',\"predictions\": proba})\n",
    "        debug and print('---')\n",
    "    return all_predictions\n",
    "\n",
    "if debug:\n",
    "    all_preds = []\n",
    "    for filepath in tqdm(filepaths, desc='Infering files'):\n",
    "        all_preds.append(infer(filepath))\n",
    "else:\n",
    "    parallel_task = (delayed(infer)(filepath) for filepath in tqdm(filepaths, desc='Infering files'))\n",
    "    all_preds = Parallel(n_jobs=os.cpu_count())(parallel_task)\n",
    "\n",
    "all_preds_flat = [item for sublist in all_preds for item in sublist]\n",
    "\n",
    "print(f\"all_preds length: {len(all_preds)}, all_preds_flat length: {len(all_preds_flat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_flat[100]['predictions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cb8d6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T04:40:57.528394Z",
     "iopub.status.busy": "2023-04-14T04:40:57.527976Z",
     "iopub.status.idle": "2023-04-14T04:40:57.575383Z",
     "shell.execute_reply": "2023-04-14T04:40:57.574650Z"
    },
    "papermill": {
     "duration": 0.055654,
     "end_time": "2023-04-14T04:40:57.577885",
     "exception": false,
     "start_time": "2023-04-14T04:40:57.522231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    pd.DataFrame({'row_id': [p['row_id'] for p in all_preds_flat]}), \n",
    "    pd.DataFrame(torch.stack([p['predictions'][0] for p in all_preds_flat]).numpy(), columns=label_encoder.classes_)\n",
    "], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594aa8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T04:40:57.588973Z",
     "iopub.status.busy": "2023-04-14T04:40:57.587752Z",
     "iopub.status.idle": "2023-04-14T04:40:57.622280Z",
     "shell.execute_reply": "2023-04-14T04:40:57.620966Z"
    },
    "papermill": {
     "duration": 0.042766,
     "end_time": "2023-04-14T04:40:57.624993",
     "exception": false,
     "start_time": "2023-04-14T04:40:57.582227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe05734",
   "metadata": {
    "papermill": {
     "duration": 0.003557,
     "end_time": "2023-04-14T04:40:57.917256",
     "exception": false,
     "start_time": "2023-04-14T04:40:57.913699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71826b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27.633397,
   "end_time": "2023-04-14T04:41:00.542795",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-14T04:40:32.909398",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
